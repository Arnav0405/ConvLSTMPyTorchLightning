{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5cbad3",
   "metadata": {},
   "source": [
    "## ConvLSTM Encoder and Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "582cd031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from training import ConvLSTM_GestureRecognitionModel\n",
    "from colorVideoDataset import ColorVideoDataset\n",
    "from datasetModule import GestureDataModule\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "956cf363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13588\\515626610.py:5: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  MODEL_CHECKPOINT = \"checkpoints\\convlstm-epoch=99-val_loss=0.87.ckpt\"\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = 'ConvLstm_final.pth'      \n",
    "DATA_ROOT_DIR = './colors'            \n",
    "BATCH_SIZE = 16\n",
    "NUM_CLASSES = 8 \n",
    "MODEL_CHECKPOINT = \"checkpoints\\convlstm-epoch=99-val_loss=0.87.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eba45f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully and set to evaluation mode.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvLSTM_GestureRecognitionModel(num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval() # Set the model to evaluation mode\n",
    "print(\"Model loaded successfully and set to evaluation mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4f6bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 320 samples and 8 classes: ['black', 'blue', 'brown', 'green', 'orange', 'red', 'white', 'yellow']\n"
     ]
    }
   ],
   "source": [
    "dataset = ColorVideoDataset(root_dir=DATA_ROOT_DIR, transform=None)\n",
    "class_names = dataset.get_class_names()\n",
    "print(f\"Dataset loaded with {len(dataset)} samples and {NUM_CLASSES} classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "722bb7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(checkpoint_path, data_dir, batch_size=16):\n",
    "    data_module = GestureDataModule(data_dir=data_dir, batch_size=batch_size)\n",
    "    data_module.setup() \n",
    "\n",
    "    test_loader = data_module.test_dataloader()\n",
    "    \n",
    "    # 1. Load the model from checkpoint\n",
    "    try:\n",
    "        model = ConvLSTM_GestureRecognitionModel.load_from_checkpoint(\n",
    "            checkpoint_path=checkpoint_path\n",
    "        )\n",
    "        print(f\"Model loaded from {checkpoint_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from checkpoint: {e}\")\n",
    "\n",
    "    # 2. Setup model for inference\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Starting inference on {len(data_module.test_dataset)} samples...\")\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # 3. Iterate through the test data and predict\n",
    "    with torch.no_grad():\n",
    "        for x, y, _ in test_loader:\n",
    "            print(x.shape)\n",
    "            # --- Input Permutation (Must match model's forward logic) ---\n",
    "            if x.shape[-1] == 3: \n",
    "                x = x.permute(0, 1, 4, 2, 3) # (B, T, H, W, C) -> (B, T, C, H, W)\n",
    "            elif x.shape[2] != 3: \n",
    "                x = x.permute(0, 2, 1, 3, 4) # (B, C, T, H, W) -> (B, T, C, H, W)\n",
    "                \n",
    "            x = x.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(x)\n",
    "            \n",
    "            # Get the predicted class index\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy()) # Collect true labels for confusion matrix/metrics\n",
    "            \n",
    "    print(\"Inference complete.\")\n",
    "    return np.array(all_preds), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c3a2c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sum of input lengths does not equal the length of the input dataset!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m predictions = \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_CHECKPOINT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./colors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m predictions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSample Predictions:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36minfer\u001b[39m\u001b[34m(checkpoint_path, data_dir, batch_size)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minfer\u001b[39m(checkpoint_path, data_dir, batch_size=\u001b[32m16\u001b[39m):\n\u001b[32m      2\u001b[39m     data_module = GestureDataModule(data_dir=data_dir, batch_size=batch_size)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mdata_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m      5\u001b[39m     test_loader = data_module.test_dataloader()\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# 1. Load the model from checkpoint\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Desktop\\traingn\\ConvLSTMPyTorchLightning\\datasetModule.py:35\u001b[39m, in \u001b[36mGestureDataModule.setup\u001b[39m\u001b[34m(self, stage)\u001b[39m\n\u001b[32m     28\u001b[39m full_dataset = ColorVideoDataset(root_dir=\u001b[38;5;28mself\u001b[39m.data_dir, transform=transform)\n\u001b[32m     29\u001b[39m \u001b[38;5;28mself\u001b[39m.train_dataset, val_set = random_split(\n\u001b[32m     30\u001b[39m         full_dataset,\n\u001b[32m     31\u001b[39m         [\u001b[38;5;28mint\u001b[39m(\u001b[32m0.8\u001b[39m * \u001b[38;5;28mlen\u001b[39m(full_dataset)), \u001b[38;5;28mlen\u001b[39m(full_dataset) - \u001b[38;5;28mint\u001b[39m(\u001b[32m0.8\u001b[39m * \u001b[38;5;28mlen\u001b[39m(full_dataset))], \n\u001b[32m     32\u001b[39m         generator=torch.Generator().manual_seed(\u001b[32m42\u001b[39m)\n\u001b[32m     33\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28mself\u001b[39m.val_dataset, \u001b[38;5;28mself\u001b[39m.test_dataset = \u001b[43mrandom_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m     \u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m     \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Desktop\\traingn\\ConvLSTMPyTorchLightning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:476\u001b[39m, in \u001b[36mrandom_split\u001b[39m\u001b[34m(dataset, lengths, generator)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# Cannot verify that dataset is Sized\u001b[39;00m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(lengths) != \u001b[38;5;28mlen\u001b[39m(dataset):  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    477\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSum of input lengths does not equal the length of the input dataset!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    478\u001b[39m     )\n\u001b[32m    480\u001b[39m indices = randperm(\u001b[38;5;28msum\u001b[39m(lengths), generator=generator).tolist()  \u001b[38;5;66;03m# type: ignore[arg-type, call-overload]\u001b[39;00m\n\u001b[32m    481\u001b[39m lengths = cast(Sequence[\u001b[38;5;28mint\u001b[39m], lengths)\n",
      "\u001b[31mValueError\u001b[39m: Sum of input lengths does not equal the length of the input dataset!"
     ]
    }
   ],
   "source": [
    "predictions = infer(MODEL_CHECKPOINT, data_dir=\"./colors\")\n",
    "\n",
    "if predictions is not None:\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(predictions[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739a2a16",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"\\n--- Confusion Matrix (Raw) ---\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a217d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names, normalize=False, title='Confusion Matrix'):\n",
    "    if normalize:\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        data = cm_normalized\n",
    "        title = title + ' (Normalized)'\n",
    "        fmt = '.2f' \n",
    "    else:\n",
    "        data = cm\n",
    "        title = title + ' (Counts)'\n",
    "        fmt = 'd'\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    sns.heatmap(data, \n",
    "                annot=True,              \n",
    "                fmt=fmt,                 \n",
    "                cmap='Blues',            \n",
    "                cbar=True,               \n",
    "                xticklabels=class_names, # X-axis labels (Predicted)\n",
    "                yticklabels=class_names) # Y-axis labels (True)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c2379",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, class_names, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c4cabd",
   "metadata": {},
   "source": [
    "### False Positives and False Negative's Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0544724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dataset.get_class_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c5568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ova_metrics(cm, class_names):\n",
    "    metrics = {}\n",
    "    # num_classes = len(class_names)\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        TP = cm[i, i]\n",
    "        \n",
    "        FN = np.sum(cm[i, :]) - TP\n",
    "        \n",
    "        FP = np.sum(cm[:, i]) - TP\n",
    "        \n",
    "        TN = np.sum(cm) - (TP + FN + FP)\n",
    "        \n",
    "        metrics[class_name] = {'TP': TP, 'TN': TN, 'FP': FP, 'FN': FN}\n",
    "        \n",
    "    df_metrics = pd.DataFrame.from_dict(metrics, orient='index')\n",
    "    return df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb2a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ova_metrics = calculate_ova_metrics(cm, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd81802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ova_metrics(df_metrics, title='One-vs-All Performance Matrix'):\n",
    "    plt.figure(figsize=(12, df_metrics.shape[0] * 1.5))\n",
    "    \n",
    "    sns.heatmap(df_metrics, \n",
    "                annot=True, \n",
    "                fmt='d',         \n",
    "                cmap='YlGnBu',    \n",
    "                linewidths=.5,    \n",
    "                linecolor='black',\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.ylabel('Class (Treated as Positive)', fontsize=14)\n",
    "    plt.xlabel('Metric', fontsize=14)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "plot_ova_metrics(df_ova_metrics, title='OvA Classification Metrics (TP, TN, FP, FN)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699841ee",
   "metadata": {},
   "source": [
    "### F1, Recall, Precision Per Class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geture-research-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
