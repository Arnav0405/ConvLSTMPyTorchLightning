{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa865b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a02e572d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 40 30\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"./colors\"\n",
    "colors_dir = os.listdir(root_dir)\n",
    "all_black = os.listdir(os.path.join(root_dir, colors_dir[0]))\n",
    "first_vid = os.listdir(os.path.join(root_dir, colors_dir[0], all_black[0]))\n",
    "print(len(colors_dir), len(all_black), len(first_vid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4c2dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorVideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for color video classification.\n",
    "    Each video is a sequence of frames, labeled by the color folder it belongs to.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None, sequence_length=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the color folders.\n",
    "            transform (callable, optional): Optional transform to be applied on frames.\n",
    "            sequence_length (int, optional): Fixed length for video sequences. If None, uses all frames.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Get all color categories\n",
    "        self.color_folders = [f for f in os.listdir(root_dir) if f.startswith('colors_')]\n",
    "        self.color_to_idx = {color.replace('colors_', ''): idx for idx, color in enumerate(self.color_folders)}\n",
    "        self.idx_to_color = {idx: color.replace('colors_', '') for idx, color in enumerate(self.color_folders)}\n",
    "        \n",
    "        # Build dataset index\n",
    "        self.video_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for color_folder in self.color_folders:\n",
    "            color_path = os.path.join(root_dir, color_folder)\n",
    "            color_name = color_folder.replace('colors_', '')\n",
    "            label = self.color_to_idx[color_name]\n",
    "            \n",
    "            # Get all video folders for this color\n",
    "            video_folders = [v for v in os.listdir(color_path) if v.startswith(color_name + '_Video_')]\n",
    "            \n",
    "            for video_folder in video_folders:\n",
    "                video_path = os.path.join(color_path, video_folder)\n",
    "                self.video_paths.append(video_path)\n",
    "                self.labels.append(label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            video_frames: Tensor of shape (T, C, H, W) where T is sequence length\n",
    "            label: Integer label for the color category\n",
    "            video_info: Dictionary with metadata\n",
    "        \"\"\"\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Get all frame files in the video folder\n",
    "        frame_files = sorted([f for f in os.listdir(video_path) if f.endswith('.jpg')])\n",
    "        \n",
    "        frames = []\n",
    "        for frame_file in frame_files:\n",
    "            frame_path = os.path.join(video_path, frame_file)\n",
    "            \n",
    "            # Load image\n",
    "            frame = cv2.imread(frame_path)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "            \n",
    "            # Apply transforms if provided\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            else:\n",
    "                # Convert to tensor and normalize to [0, 1]\n",
    "                frame = torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0\n",
    "            \n",
    "            frames.append(frame)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        video_frames = torch.stack(frames)  # Shape: (T, C, H, W)\n",
    "        \n",
    "        # Handle sequence length\n",
    "        if self.sequence_length is not None:\n",
    "            if len(video_frames) >= self.sequence_length:\n",
    "                # Randomly sample frames or take first N frames\n",
    "                video_frames = video_frames[:self.sequence_length]\n",
    "            else:\n",
    "                # Pad with last frame if sequence is shorter\n",
    "                padding_needed = self.sequence_length - len(video_frames)\n",
    "                last_frame = video_frames[-1].unsqueeze(0)\n",
    "                padding = last_frame.repeat(padding_needed, 1, 1, 1)\n",
    "                video_frames = torch.cat([video_frames, padding], dim=0)\n",
    "        \n",
    "        # Create metadata\n",
    "        video_info = {\n",
    "            'video_path': video_path,\n",
    "            'color': self.idx_to_color[label],\n",
    "            'num_frames': len(frame_files),\n",
    "            'video_folder': os.path.basename(video_path)\n",
    "        }\n",
    "        \n",
    "        return video_frames, label, video_info\n",
    "    \n",
    "    def get_class_names(self):\n",
    "        \"\"\"Returns list of color class names\"\"\"\n",
    "        return [self.idx_to_color[i] for i in range(len(self.idx_to_color))]\n",
    "    \n",
    "    def get_num_classes(self):\n",
    "        \"\"\"Returns number of color classes\"\"\"\n",
    "        return len(self.color_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdc17355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 320\n",
      "Number of classes: 8\n",
      "Class names: ['black', 'blue', 'brown', 'green', 'orange', 'red', 'white', 'yellow']\n",
      "\n",
      "Sample video info:\n",
      "Video path: ./colors\\colors_black\\black_Video_1\n",
      "Color: black\n",
      "Label: 0\n",
      "Video frames shape: torch.Size([15, 3, 480, 640])\n",
      "Original frames: 30\n",
      "\n",
      "Batch 0:\n",
      "Batch frames shape: torch.Size([4, 15, 3, 480, 640])\n",
      "Batch labels: tensor([4, 0, 4, 4])\n",
      "Colors in batch: ['orange', 'black', 'orange', 'orange']\n",
      "Video folders: ['orange_Video_1', 'black_Video_12', 'orange_Video_16', 'orange_Video_18']\n"
     ]
    }
   ],
   "source": [
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dataset\n",
    "    dataset = ColorVideoDataset(root_dir=\"./colors\", sequence_length=15)\n",
    "    \n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    print(f\"Number of classes: {dataset.get_num_classes()}\")\n",
    "    print(f\"Class names: {dataset.get_class_names()}\")\n",
    "    \n",
    "    # Test loading a sample\n",
    "    video_frames, label, video_info = dataset[0]\n",
    "    print(f\"\\nSample video info:\")\n",
    "    print(f\"Video path: {video_info['video_path']}\")\n",
    "    print(f\"Color: {video_info['color']}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Video frames shape: {video_frames.shape}\")\n",
    "    print(f\"Original frames: {video_info['num_frames']}\")\n",
    "    \n",
    "    # Create custom collate function for DataLoader to handle metadata\n",
    "    def custom_collate_fn(batch):\n",
    "        videos, labels, infos = zip(*batch)\n",
    "        videos = torch.stack(videos)\n",
    "        labels = torch.tensor(labels)\n",
    "        return videos, labels, infos\n",
    "    \n",
    "    # Create DataLoader for batching\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, \n",
    "                          collate_fn=custom_collate_fn, num_workers=0)\n",
    "    \n",
    "    # Test batch loading\n",
    "    for batch_idx, (batch_frames, batch_labels, batch_info) in enumerate(dataloader):\n",
    "        print(f\"\\nBatch {batch_idx}:\")\n",
    "        print(f\"Batch frames shape: {batch_frames.shape}\")\n",
    "        print(f\"Batch labels: {batch_labels}\")\n",
    "        print(f\"Colors in batch: {[info['color'] for info in batch_info]}\")\n",
    "        print(f\"Video folders: {[info['video_folder'] for info in batch_info]}\")\n",
    "        if batch_idx == 0:  # Only show first batch\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 320\n",
      "Train dataset size: 256\n",
      "Validation dataset size: 64\n",
      "number of training labels: 256\n",
      "\n",
      "Training set label distribution:\n",
      "  red: 32 videos\n",
      "  white: 33 videos\n",
      "  orange: 32 videos\n",
      "  yellow: 33 videos\n",
      "  green: 27 videos\n",
      "  brown: 32 videos\n",
      "  blue: 32 videos\n",
      "  black: 35 videos\n",
      "\n",
      "Train batches: 32\n",
      "Validation batches: 8\n",
      "\n",
      "Dataset statistics:\n",
      "Video resolution: torch.Size([480, 640]) (H x W)\n",
      "Sequence length: 30\n",
      "Color channels: 3\n",
      "Sequence length: 30\n",
      "Color channels: 3\n",
      "Pixel value range: [0.000, 1.000]\n",
      "Pixel value range: [0.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "# Advanced usage: Train/Validation split and data statistics\n",
    "from torch.utils.data import random_split\n",
    "from collections import Counter\n",
    "\n",
    "# Create dataset\n",
    "full_dataset = ColorVideoDataset(root_dir=\"./colors\", sequence_length=30)\n",
    "\n",
    "# Create train/validation split (80/20)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Full dataset size: {len(full_dataset)}\")\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Check label distribution in training set\n",
    "train_labels = [full_dataset.labels[i] for i in train_dataset.indices]\n",
    "label_counts = Counter(train_labels)\n",
    "print(f\"\\nTraining set label distribution:\")\n",
    "for label, count in label_counts.items():\n",
    "    color_name = full_dataset.idx_to_color[label]\n",
    "    print(f\"  {color_name}: {count} videos\")\n",
    "\n",
    "# Create data loaders with proper collate function\n",
    "def video_collate_fn(batch):\n",
    "    videos, labels, infos = zip(*batch)\n",
    "    videos = torch.stack(videos)\n",
    "    labels = torch.tensor(labels)\n",
    "    return videos, labels, infos\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, \n",
    "                         collate_fn=video_collate_fn, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, \n",
    "                       collate_fn=video_collate_fn, num_workers=0)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Example: Calculate dataset statistics\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"Video resolution: {full_dataset[0][0].shape[2:]} (H x W)\")\n",
    "print(f\"Sequence length: {full_dataset[0][0].shape[0]}\")\n",
    "print(f\"Color channels: {full_dataset[0][0].shape[1]}\")\n",
    "print(f\"Pixel value range: [{full_dataset[0][0].min():.3f}, {full_dataset[0][0].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = ColorVideoDataset(root_dir=\"./colors\", sequence_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8897a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Define transforms for data augmentation and preprocessing\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "\n",
    "def create_video_transform(image_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Create a transform pipeline for video frames\n",
    "    \"\"\"\n",
    "    transforms_list = [\n",
    "        lambda x: Image.fromarray(x),  # Convert numpy array to PIL Image\n",
    "        Resize(image_size),\n",
    "        ToTensor(),  # Converts to [0, 1] and changes to (C, H, W)\n",
    "    ]\n",
    "    \n",
    "    return Compose(transforms_list)\n",
    "\n",
    "transform = create_video_transform(image_size=(128, 128))\n",
    "dataset_with_transforms = ColorVideoDataset(\n",
    "    root_dir=\"./colors\", \n",
    "    transform=transform,\n",
    "    sequence_length=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geture-research-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
