{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b348d8c",
   "metadata": {},
   "source": [
    "# Initial Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1273de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from colorVideoDataset import ColorVideoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49168ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = ColorVideoDataset('./colors', sequence_length=30)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd470b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = random_split(DATASET, [int(0.8 * len(DATASET)), len(DATASET) - int(0.8 * len(DATASET))])\n",
    "test_dataset, val_dataset = random_split(test_dataset, [int(0.5 * len(test_dataset)), len(test_dataset) - int(0.5 * len(test_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7824f7d8",
   "metadata": {},
   "source": [
    "# PyTorch 3D ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ee457ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from models.ThreeDResNet import get_3dResNet, get_resnet_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dbb3203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Arnav Waghdhare/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (blocks): ModuleList(\n",
      "    (0): ResNetBasicStem(\n",
      "      (conv): Conv3d(3, 64, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False)\n",
      "      (norm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "      (pool): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=[0, 1, 1], dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): ResStage(\n",
      "      (res_blocks): ModuleList(\n",
      "        (0): ResBlock(\n",
      "          (branch1_conv): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(np.int64(1), np.int64(1), np.int64(1)), bias=False)\n",
      "          (branch1_norm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_a): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "        (1-2): 2 x ResBlock(\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_a): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ResStage(\n",
      "      (res_blocks): ModuleList(\n",
      "        (0): ResBlock(\n",
      "          (branch1_conv): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(np.int64(1), np.int64(2), np.int64(2)), bias=False)\n",
      "          (branch1_norm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_a): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "        (1-3): 3 x ResBlock(\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_a): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): ResStage(\n",
      "      (res_blocks): ModuleList(\n",
      "        (0): ResBlock(\n",
      "          (branch1_conv): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(np.int64(1), np.int64(2), np.int64(2)), bias=False)\n",
      "          (branch1_norm): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(512, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "            (norm_a): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "        (1-5): 5 x ResBlock(\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(1024, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "            (norm_a): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): ResStage(\n",
      "      (res_blocks): ModuleList(\n",
      "        (0): ResBlock(\n",
      "          (branch1_conv): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(np.int64(1), np.int64(2), np.int64(2)), bias=False)\n",
      "          (branch1_norm): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(1024, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "            (norm_a): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(512, 512, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "        (1-2): 2 x ResBlock(\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(2048, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "            (norm_a): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(512, 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): ResNetBasicHead(\n",
      "      (pool): AvgPool3d(kernel_size=(8, 7, 7), stride=(1, 1, 1), padding=(0, 0, 0))\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (proj): Linear(in_features=2048, out_features=8, bias=True)\n",
      "      (output_pool): AdaptiveAvgPool3d(output_size=1)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "MODEL = get_3dResNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097d56fe",
   "metadata": {},
   "source": [
    "Train, Test, Val Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e3a0cc",
   "metadata": {},
   "source": [
    "## Dataloader and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f516fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, subset_ratio : float | None = 0.1, batch_size : int = 2):\n",
    "    transform = get_resnet_transformer()\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        videos = []\n",
    "        labels = []\n",
    "        for video, label, _ in batch:\n",
    "            video = video.permute(1, 0, 2, 3)\n",
    "            video = transform({\"video\": video})[\"video\"]\n",
    "            videos.append(video)\n",
    "            labels.append(torch.tensor(label, dtype=torch.long))\n",
    "        \n",
    "        videos = torch.stack(videos)\n",
    "        labels = torch.stack(labels)\n",
    "        return videos, labels\n",
    "    \n",
    "    if subset_ratio is not None:\n",
    "        num_samples = int(len(dataset) * subset_ratio) \n",
    "        subset_indices = list(range(num_samples))\n",
    "        subset = Subset(dataset, subset_indices)\n",
    "        dataloader = DataLoader(subset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    else:\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b58341e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_loop(model, dataloader, criterion = None) -> tuple:\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    for videos, labels in dataloader:\n",
    "        videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        all_preds.extend(outputs.argmax(dim=1).cpu().detach().numpy())\n",
    "        all_labels.extend(labels.cpu().detach().numpy())\n",
    "        \n",
    "    val_accuracy = accuracy_score(all_preds, all_labels)\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "\n",
    "    return (val_accuracy, avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_dataloader, val_dataloader: DataLoader | None = None, \n",
    "                  epochs=5, learning_rate=2e-4, early_stopping_patience=10,\n",
    "                  checkpoint_path='best_model_3DResNet.pth', lr_patience=2):\n",
    "\n",
    "    results_dict = {\n",
    "        'time_per_batch': [],\n",
    "        'time_per_epoch': [],\n",
    "        'train_accuracy': [],\n",
    "        'train_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_loss': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'learning_rate': [],\n",
    "    }\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # ReduceLROnPlateau - tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=lr_patience, verbose=True, min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    # EarlyStopping - tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    epoch_pbar = tqdm(range(epochs), desc=\"Training Epochs\")\n",
    "\n",
    "    for epoch in epoch_pbar:\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        epoch_start = time()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for videos, labels in train_dataloader:\n",
    "            videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            all_preds.extend(outputs.argmax(dim=1).cpu().detach().numpy())\n",
    "            all_labels.extend(labels.cpu().detach().numpy())\n",
    "        \n",
    "        epoch_time = time() - epoch_start\n",
    "        results_dict['time_per_epoch'].append(epoch_time)\n",
    "        \n",
    "        # MetricsCallback - tracks precision, recall, f1\n",
    "        results_dict['train_accuracy'].append(accuracy_score(all_labels, all_preds))\n",
    "        results_dict['precision'].append(precision_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
    "        results_dict['recall'].append(recall_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
    "        results_dict['f1'].append(f1_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_dataloader)\n",
    "        results_dict['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        val_accuracy, val_loss = infer_loop(model, val_dataloader, criterion)\n",
    "        \n",
    "        results_dict['val_accuracy'].append(val_accuracy)\n",
    "        results_dict['val_loss'].append(val_loss)\n",
    "        results_dict['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # ReduceLROnPlateau callback\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # ModelCheckpoint - tf.keras.callbacks.ModelCheckpoint('best_model_TVN.h5', monitor='val_loss', save_best_only=True)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            epoch_pbar.write(f\"✓ Epoch {epoch+1}: Saved best model with val_loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        epoch_pbar.set_postfix({\n",
    "            'train_loss': f\"{avg_train_loss:.4f}\",\n",
    "            'val_loss': f\"{val_loss:.4f}\",\n",
    "            'val_acc': f\"{val_accuracy:.4f}\",\n",
    "            'lr': f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
    "        })\n",
    "        \n",
    "        # EarlyStopping check\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            epoch_pbar.write(f\"\\n⚠ Early stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    epoch_pbar.write(f\"\\n✓ Loaded best model from {checkpoint_path}\")\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = get_dataloader(DATASET, subset_ratio=0.1, batch_size=16)\n",
    "x, y = next(iter(dataloader))\n",
    "print(x.shape, y.shape)\n",
    "del x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95223ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader =  get_dataloader(train_dataset, subset_ratio=None, batch_size=16)\n",
    "val_dataloader =  get_dataloader(val_dataset, subset_ratio=None, batch_size=16)\n",
    "test_dataloader =  get_dataloader(test_dataset, subset_ratio=None, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a57a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = training_loop(\n",
    "    MODEL, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    epochs=100, \n",
    "    learning_rate=2e-4,\n",
    "    early_stopping_patience=10,              \n",
    "    checkpoint_path='best_model_3DResNet.pth',  \n",
    "    lr_patience=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe37e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './models/trained/3dResnet_Trained.pth'\n",
    "torch.save(MODEL.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(results, orient=\"index\").T\n",
    "results_df.to_csv(\"3D_ResNet_Results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b7b57",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1dc20aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9375, 0.259842649102211)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL.load_state_dict(torch.load(\"models\\\\trained_models\\\\3dResnet_Trained.pth\", map_location=DEVICE))\n",
    "MODEL.to(DEVICE)\n",
    "MODEL.eval()\n",
    "test_acc, test_loss = infer_loop(MODEL, test_dataloader)\n",
    "test_acc, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc92cc",
   "metadata": {},
   "source": [
    "# TensorFlow TinyVideoNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87e7c33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow_hub\\__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\models\\TinyVideoNet.py:7: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as k\n",
    "import numpy as np\n",
    "from models.TinyVideoNet import TinyVideoNetTransfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41af0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_data_generator(torch_dataset):\n",
    "    def generator():\n",
    "        for i in range(len(torch_dataset)):\n",
    "            video, label, _ = torch_dataset[i]\n",
    "            label = tf.one_hot(label, depth=8)\n",
    "            yield video, label\n",
    "    return generator\n",
    "\n",
    "def create_dataset(torch_ds, batch_size=4):\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        tf_data_generator(torch_ds),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(30, 3, 480, 640), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(8,), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "    return ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF Datasets\n",
    "tf_train = create_dataset(train_dataset, batch_size=4)\n",
    "tf_val = create_dataset(val_dataset, batch_size=4)\n",
    "tf_test = create_dataset(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2618f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_handle = 'https://kaggle.com/models/google/tiny-video-net/frameworks/TensorFlow1/variations/tvn1/versions/1'\n",
    "tf_model = TinyVideoNetTransfer(model_handle, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed7f3988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCallback(k.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.time_per_epoch = []  # Track epoch times\n",
    "        self.epoch_start_time = None\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Calculate epoch time\n",
    "        epoch_time = time() - self.epoch_start_time\n",
    "        self.time_per_epoch.append(epoch_time)\n",
    "        logs['time_per_epoch'] = epoch_time\n",
    "\n",
    "# Create the callback\n",
    "metrics_callback = MetricsCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28483f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.compile(\n",
    "    optimizer=k.optimizers.Adam(learning_rate=3e-4), \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        k.metrics.Precision(),\n",
    "        k.metrics.Recall(), \n",
    "        k.metrics.F1Score(average='macro')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc9cbb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae00aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (8, 30, 3, 480, 640)\n",
      "y.shape: (8, 8)\n",
      "y.dtype: <dtype: 'float32'>\n",
      "Sample label: [1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(tf_train))\n",
    "print(\"x.shape:\", x.shape)  # Should be (B, 30, 3, 480, 640)\n",
    "print(\"y.shape:\", y.shape)  # Should be (B,)\n",
    "print(\"y.dtype:\", y.dtype)  # Should be int32 or int64\n",
    "print(\"Sample label:\", y[0].numpy())  # Should be integer in [0, 7]\n",
    "\n",
    "del x, y, DATASET, train_dataset, val_dataset, test_dataset, DEVICE\n",
    "del torch, nn, tqdm, accuracy_score, precision_score, recall_score, f1_score\n",
    "del DataLoader, Subset, random_split, ColorVideoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa8c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "      4/Unknown \u001b[1m13s\u001b[0m 3s/step - accuracy: 0.1016 - f1_score: 0.0417 - loss: 3.7131 - precision: 0.1727 - recall: 0.1016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:164: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 8s/step - accuracy: 0.0938 - f1_score: 0.0357 - loss: 3.7818 - precision: 0.1500 - recall: 0.0938 - val_accuracy: 0.1562 - val_f1_score: 0.0338 - val_loss: 2.4679 - val_precision: 0.5000 - val_recall: 0.0312 - learning_rate: 3.0000e-04 - time_per_epoch: 27.0586\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4s/step - accuracy: 0.2500 - f1_score: 0.1903 - loss: 2.9133 - precision: 0.3333 - recall: 0.1562 - val_accuracy: 0.0938 - val_f1_score: 0.0474 - val_loss: 2.3962 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 3.0000e-04 - time_per_epoch: 13.7464\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3s/step - accuracy: 0.0625 - f1_score: 0.0536 - loss: 3.0031 - precision: 0.0714 - recall: 0.0312 - val_accuracy: 0.0938 - val_f1_score: 0.0474 - val_loss: 2.3131 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 3.0000e-04 - time_per_epoch: 12.4654\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4s/step - accuracy: 0.1875 - f1_score: 0.1288 - loss: 2.6060 - precision: 0.1875 - recall: 0.0938 - val_accuracy: 0.2812 - val_f1_score: 0.1930 - val_loss: 2.1934 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 3.0000e-04 - time_per_epoch: 12.8367\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4s/step - accuracy: 0.1562 - f1_score: 0.1130 - loss: 2.4438 - precision: 0.1875 - recall: 0.0938 - val_accuracy: 0.2500 - val_f1_score: 0.1319 - val_loss: 2.1702 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 3.0000e-04 - time_per_epoch: 12.7359\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4s/step - accuracy: 0.1875 - f1_score: 0.1252 - loss: 2.4479 - precision: 0.0833 - recall: 0.0312 - val_accuracy: 0.2500 - val_f1_score: 0.1635 - val_loss: 2.1179 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 3.0000e-04 - time_per_epoch: 12.8424\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4s/step - accuracy: 0.2500 - f1_score: 0.1925 - loss: 2.1457 - precision: 0.4545 - recall: 0.1562 - val_accuracy: 0.2188 - val_f1_score: 0.1419 - val_loss: 2.0920 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 3.0000e-04 - time_per_epoch: 13.6958\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3s/step - accuracy: 0.3125 - f1_score: 0.1724 - loss: 2.5758 - precision: 0.5000 - recall: 0.1250 - val_accuracy: 0.1875 - val_f1_score: 0.1077 - val_loss: 2.1130 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 3.0000e-04 - time_per_epoch: 11.9202\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4s/step - accuracy: 0.2812 - f1_score: 0.1919 - loss: 2.0639 - precision: 0.3000 - recall: 0.0938 - val_accuracy: 0.1875 - val_f1_score: 0.1126 - val_loss: 2.0613 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 3.0000e-04 - time_per_epoch: 13.2972\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4s/step - accuracy: 0.4062 - f1_score: 0.2991 - loss: 1.9287 - precision: 0.6000 - recall: 0.1875 - val_accuracy: 0.2188 - val_f1_score: 0.1228 - val_loss: 2.0203 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 3.0000e-04 - time_per_epoch: 13.0050\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4s/step - accuracy: 0.3125 - f1_score: 0.2421 - loss: 1.7458 - precision: 0.3333 - recall: 0.1250 - val_accuracy: 0.2188 - val_f1_score: 0.1913 - val_loss: 2.0321 - val_precision: 1.0000 - val_recall: 0.0312 - learning_rate: 3.0000e-04 - time_per_epoch: 12.5154\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4s/step - accuracy: 0.2188 - f1_score: 0.1535 - loss: 2.2870 - precision: 0.2500 - recall: 0.0625 - val_accuracy: 0.2812 - val_f1_score: 0.2503 - val_loss: 1.9885 - val_precision: 1.0000 - val_recall: 0.0312 - learning_rate: 3.0000e-04 - time_per_epoch: 12.8964\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4s/step - accuracy: 0.2188 - f1_score: 0.1715 - loss: 1.9517 - precision: 0.3636 - recall: 0.1250 - val_accuracy: 0.2500 - val_f1_score: 0.2113 - val_loss: 1.9699 - val_precision: 1.0000 - val_recall: 0.0312 - learning_rate: 3.0000e-04 - time_per_epoch: 13.4913\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4s/step - accuracy: 0.2812 - f1_score: 0.2148 - loss: 1.9453 - precision: 0.3636 - recall: 0.1250 - val_accuracy: 0.2188 - val_f1_score: 0.1266 - val_loss: 1.9770 - val_precision: 1.0000 - val_recall: 0.0312 - learning_rate: 3.0000e-04 - time_per_epoch: 14.0113\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4s/step - accuracy: 0.2500 - f1_score: 0.2431 - loss: 2.1296 - precision: 0.4286 - recall: 0.0938 - val_accuracy: 0.2188 - val_f1_score: 0.1266 - val_loss: 2.0174 - val_precision: 1.0000 - val_recall: 0.0938 - learning_rate: 3.0000e-04 - time_per_epoch: 13.9364\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4s/step - accuracy: 0.2812 - f1_score: 0.1520 - loss: 1.8863 - precision: 0.4167 - recall: 0.1562 - val_accuracy: 0.2188 - val_f1_score: 0.1333 - val_loss: 1.9881 - val_precision: 1.0000 - val_recall: 0.0938 - learning_rate: 2.7000e-04 - time_per_epoch: 13.6328\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4s/step - accuracy: 0.3125 - f1_score: 0.1656 - loss: 2.0503 - precision: 0.3000 - recall: 0.0938 - val_accuracy: 0.2812 - val_f1_score: 0.2193 - val_loss: 1.8830 - val_precision: 1.0000 - val_recall: 0.0312 - learning_rate: 2.7000e-04 - time_per_epoch: 13.4043\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4s/step - accuracy: 0.2812 - f1_score: 0.2599 - loss: 1.8760 - precision: 0.4000 - recall: 0.0625 - val_accuracy: 0.3125 - val_f1_score: 0.2294 - val_loss: 1.8621 - val_precision: 1.0000 - val_recall: 0.0312 - learning_rate: 2.7000e-04 - time_per_epoch: 12.7493\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4s/step - accuracy: 0.2812 - f1_score: 0.2023 - loss: 1.6372 - precision: 1.0000 - recall: 0.1250 - val_accuracy: 0.3125 - val_f1_score: 0.2401 - val_loss: 1.8774 - val_precision: 1.0000 - val_recall: 0.0312 - learning_rate: 2.7000e-04 - time_per_epoch: 13.0691\n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3021 - f1_score: 0.1755 - loss: 1.8999 - precision: 0.2333 - recall: 0.0339       "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = \u001b[43mtf_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtf_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mval_loss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./models/trained_models/TVN_best_model.weights.h5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mval_loss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_weights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mval_loss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetrics_callback\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:423\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_eval_epoch_iterator\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    413\u001b[39m     \u001b[38;5;28mself\u001b[39m._eval_epoch_iterator = TFEpochIterator(\n\u001b[32m    414\u001b[39m         x=val_x,\n\u001b[32m    415\u001b[39m         y=val_y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    421\u001b[39m         shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    422\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m val_logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m val_logs = {\n\u001b[32m    434\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mval_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs.items()\n\u001b[32m    435\u001b[39m }\n\u001b[32m    436\u001b[39m epoch_logs.update(val_logs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:511\u001b[39m, in \u001b[36mTensorFlowTrainer.evaluate\u001b[39m\u001b[34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    510\u001b[39m     callbacks.on_test_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m     callbacks.on_test_batch_end(end_step, logs)\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_evaluating:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "history = tf_model.fit(\n",
    "    tf_train,\n",
    "    validation_data=tf_val, \n",
    "    epochs=100, \n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10), \n",
    "        tf.keras.callbacks.ModelCheckpoint('./models/trained_models/TVN_best_model.weights.h5', monitor='val_loss', save_best_only=True, save_weights_only=True), \n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.9, min_lr=1e-7), \n",
    "        metrics_callback\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40c068f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.09375,\n",
       "  0.15625,\n",
       "  0.1875,\n",
       "  0.09375,\n",
       "  0.125,\n",
       "  0.21875,\n",
       "  0.1875,\n",
       "  0.15625,\n",
       "  0.28125,\n",
       "  0.28125,\n",
       "  0.25],\n",
       " 'f1_score': [0.052083320915699005,\n",
       "  0.07670453190803528,\n",
       "  0.07131408900022507,\n",
       "  0.05929486081004143,\n",
       "  0.08571426570415497,\n",
       "  0.16856059432029724,\n",
       "  0.08012819290161133,\n",
       "  0.0694444328546524,\n",
       "  0.1409090757369995,\n",
       "  0.1775640845298767,\n",
       "  0.1577380746603012],\n",
       " 'loss': [3.255795955657959,\n",
       "  2.8296751976013184,\n",
       "  2.4811947345733643,\n",
       "  2.2535057067871094,\n",
       "  2.7205843925476074,\n",
       "  2.472210645675659,\n",
       "  2.9941649436950684,\n",
       "  2.7692158222198486,\n",
       "  2.746459722518921,\n",
       "  2.4065213203430176,\n",
       "  2.6997034549713135],\n",
       " 'precision': [0.04545454680919647,\n",
       "  0.1818181872367859,\n",
       "  0.1666666716337204,\n",
       "  0.1428571492433548,\n",
       "  0.1764705926179886,\n",
       "  0.3571428656578064,\n",
       "  0.1818181872367859,\n",
       "  0.1111111119389534,\n",
       "  0.2666666805744171,\n",
       "  0.23529411852359772,\n",
       "  0.29411765933036804],\n",
       " 'recall': [0.03125,\n",
       "  0.0625,\n",
       "  0.0625,\n",
       "  0.0625,\n",
       "  0.09375,\n",
       "  0.15625,\n",
       "  0.125,\n",
       "  0.0625,\n",
       "  0.125,\n",
       "  0.125,\n",
       "  0.15625],\n",
       " 'val_accuracy': [0.125,\n",
       "  0.0625,\n",
       "  0.0625,\n",
       "  0.0625,\n",
       "  0.0625,\n",
       "  0.0625,\n",
       "  0.0625,\n",
       "  0.0625,\n",
       "  0.0625,\n",
       "  0.0625,\n",
       "  0.0625],\n",
       " 'val_f1_score': [0.06397305428981781,\n",
       "  0.014705880545079708,\n",
       "  0.014705880545079708,\n",
       "  0.014705880545079708,\n",
       "  0.014705880545079708,\n",
       "  0.014705880545079708,\n",
       "  0.014705880545079708,\n",
       "  0.014705880545079708,\n",
       "  0.014705880545079708,\n",
       "  0.014705880545079708,\n",
       "  0.014705880545079708],\n",
       " 'val_loss': [2.358474016189575,\n",
       "  2.5206966400146484,\n",
       "  2.67954421043396,\n",
       "  2.6863536834716797,\n",
       "  2.6632676124572754,\n",
       "  2.6594653129577637,\n",
       "  2.6543197631835938,\n",
       "  2.6537630558013916,\n",
       "  2.653127670288086,\n",
       "  2.6529293060302734,\n",
       "  2.652724027633667],\n",
       " 'val_precision': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'val_recall': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'learning_rate': [0.0003000000142492354,\n",
       "  0.0003000000142492354,\n",
       "  0.0003000000142492354,\n",
       "  3.000000106112566e-05,\n",
       "  3.000000106112566e-05,\n",
       "  3.000000106112566e-06,\n",
       "  3.000000106112566e-06,\n",
       "  3.000000106112566e-07,\n",
       "  3.000000106112566e-07,\n",
       "  1.0000000116860974e-07,\n",
       "  1.0000000116860974e-07],\n",
       " 'time_per_epoch': [30.477773904800415,\n",
       "  14.537723779678345,\n",
       "  13.72228193283081,\n",
       "  19.24220299720764,\n",
       "  26.44040036201477,\n",
       "  13.990036010742188,\n",
       "  15.134760856628418,\n",
       "  14.16549301147461,\n",
       "  14.139251947402954,\n",
       "  13.87432336807251,\n",
       "  14.714041471481323]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfff106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(history.history, orient=\"index\").T\n",
    "# results_df.to_csv(\"TVN_results_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86d25261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>loss</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_f1_score</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>time_per_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.09375</td>\n",
       "      <td>0.052083</td>\n",
       "      <td>3.255796</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.063973</td>\n",
       "      <td>2.358474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000e-04</td>\n",
       "      <td>30.477774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.15625</td>\n",
       "      <td>0.076705</td>\n",
       "      <td>2.829675</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>2.520697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000e-04</td>\n",
       "      <td>14.537724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.18750</td>\n",
       "      <td>0.071314</td>\n",
       "      <td>2.481195</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>2.679544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000e-04</td>\n",
       "      <td>13.722282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.09375</td>\n",
       "      <td>0.059295</td>\n",
       "      <td>2.253506</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>2.686354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000e-05</td>\n",
       "      <td>19.242203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.12500</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>2.720584</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.09375</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>2.663268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000e-05</td>\n",
       "      <td>26.440400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.21875</td>\n",
       "      <td>0.168561</td>\n",
       "      <td>2.472211</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.15625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>2.659465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000e-06</td>\n",
       "      <td>13.990036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.18750</td>\n",
       "      <td>0.080128</td>\n",
       "      <td>2.994165</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>2.654320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000e-06</td>\n",
       "      <td>15.134761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.15625</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>2.769216</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>2.653763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000e-07</td>\n",
       "      <td>14.165493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.28125</td>\n",
       "      <td>0.140909</td>\n",
       "      <td>2.746460</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>2.653128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000e-07</td>\n",
       "      <td>14.139252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.28125</td>\n",
       "      <td>0.177564</td>\n",
       "      <td>2.406521</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>2.652929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>13.874323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.157738</td>\n",
       "      <td>2.699703</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.15625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>2.652724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>14.714041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score      loss  precision   recall  val_accuracy  \\\n",
       "0    0.09375  0.052083  3.255796   0.045455  0.03125        0.1250   \n",
       "1    0.15625  0.076705  2.829675   0.181818  0.06250        0.0625   \n",
       "2    0.18750  0.071314  2.481195   0.166667  0.06250        0.0625   \n",
       "3    0.09375  0.059295  2.253506   0.142857  0.06250        0.0625   \n",
       "4    0.12500  0.085714  2.720584   0.176471  0.09375        0.0625   \n",
       "5    0.21875  0.168561  2.472211   0.357143  0.15625        0.0625   \n",
       "6    0.18750  0.080128  2.994165   0.181818  0.12500        0.0625   \n",
       "7    0.15625  0.069444  2.769216   0.111111  0.06250        0.0625   \n",
       "8    0.28125  0.140909  2.746460   0.266667  0.12500        0.0625   \n",
       "9    0.28125  0.177564  2.406521   0.235294  0.12500        0.0625   \n",
       "10   0.25000  0.157738  2.699703   0.294118  0.15625        0.0625   \n",
       "\n",
       "    val_f1_score  val_loss  val_precision  val_recall  learning_rate  \\\n",
       "0       0.063973  2.358474            0.0         0.0   3.000000e-04   \n",
       "1       0.014706  2.520697            0.0         0.0   3.000000e-04   \n",
       "2       0.014706  2.679544            0.0         0.0   3.000000e-04   \n",
       "3       0.014706  2.686354            0.0         0.0   3.000000e-05   \n",
       "4       0.014706  2.663268            0.0         0.0   3.000000e-05   \n",
       "5       0.014706  2.659465            0.0         0.0   3.000000e-06   \n",
       "6       0.014706  2.654320            0.0         0.0   3.000000e-06   \n",
       "7       0.014706  2.653763            0.0         0.0   3.000000e-07   \n",
       "8       0.014706  2.653128            0.0         0.0   3.000000e-07   \n",
       "9       0.014706  2.652929            0.0         0.0   1.000000e-07   \n",
       "10      0.014706  2.652724            0.0         0.0   1.000000e-07   \n",
       "\n",
       "    time_per_epoch  \n",
       "0        30.477774  \n",
       "1        14.537724  \n",
       "2        13.722282  \n",
       "3        19.242203  \n",
       "4        26.440400  \n",
       "5        13.990036  \n",
       "6        15.134761  \n",
       "7        14.165493  \n",
       "8        14.139252  \n",
       "9        13.874323  \n",
       "10       14.714041  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5409a4cf",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e547d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.build(input_shape=(None, 16, 3, 480, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e136167f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nTypeError: `generator` yielded an element of shape (16, 3, 480, 640) where an element of shape (30, 3, 480, 640) was expected.\nTraceback (most recent call last):\n\n  File \"c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 235, in generator_py_func\n    raise TypeError(\n\nTypeError: `generator` yielded an element of shape (16, 3, 480, 640) where an element of shape (30, 3, 480, 640) was expected.\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_4190]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m tf_model.load_weights(\u001b[33m'\u001b[39m\u001b[33mmodels/trained_models/TVN_best_model.weights.h5\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Evaluate the model on the test dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m test_loss, test_accuracy = \u001b[43mtf_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nTypeError: `generator` yielded an element of shape (16, 3, 480, 640) where an element of shape (30, 3, 480, 640) was expected.\nTraceback (most recent call last):\n\n  File \"c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 235, in generator_py_func\n    raise TypeError(\n\nTypeError: `generator` yielded an element of shape (16, 3, 480, 640) where an element of shape (30, 3, 480, 640) was expected.\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_4190]"
     ]
    }
   ],
   "source": [
    "# Load the best model weights\n",
    "tf_model.load_weights('models/trained_models/TVN_best_model.weights.h5')\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = tf_model.evaluate(tf_test)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc733eeb",
   "metadata": {},
   "source": [
    "# PyTorch VideoMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19870247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models.VideoMAE_SL import VideoMAEClassifier\n",
    "from transformers import VideoMAEImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab638c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 184/184 [00:00<00:00, 368.99it/s, Materializing param=layernorm.weight]                                 \n",
      "VideoMAEModel LOAD REPORT from: MCG-NJU/videomae-base\n",
      "Key                                                                  | Status     |  | \n",
      "---------------------------------------------------------------------+------------+--+-\n",
      "decoder.decoder_layers.{0, 1, 2, 3}.attention.attention.q_bias       | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.attention.attention.v_bias       | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.attention.attention.key.weight   | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.attention.output.dense.bias      | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.intermediate.dense.bias          | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.layernorm_after.weight           | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.attention.attention.value.weight | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.layernorm_after.bias             | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.intermediate.dense.weight        | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.attention.attention.query.weight | UNEXPECTED |  | \n",
      "encoder_to_decoder.weight                                            | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.output.dense.weight              | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.layernorm_before.weight          | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.layernorm_before.bias            | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.attention.output.dense.weight    | UNEXPECTED |  | \n",
      "decoder.head.weight                                                  | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.output.dense.bias                | UNEXPECTED |  | \n",
      "decoder.norm.bias                                                    | UNEXPECTED |  | \n",
      "mask_token                                                           | UNEXPECTED |  | \n",
      "decoder.head.bias                                                    | UNEXPECTED |  | \n",
      "decoder.norm.weight                                                  | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "MODEL = VideoMAEClassifier(num_classes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cdb415",
   "metadata": {},
   "source": [
    "## Dataloader and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4876d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, subset_ratio : float | None = 0.1, batch_size : int = 2):\n",
    "    processor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        videos = []\n",
    "        labels = []\n",
    "        for video, label, _ in batch:\n",
    "            video = video.permute(0, 2, 3, 1)\n",
    "            video_list = [video[i].numpy() for i in range(video.shape[0])]\n",
    "\n",
    "            video = processor(video_list, return_tensors=\"pt\", do_normalize=False, do_rescale=False)[\"pixel_values\"].squeeze(0) \n",
    "            # video = processor(video, return_tensors=\"pt\", do_normalize=False, do_convert_rgb=False, do_center_crop=False)[\"pixel_values\"].squeeze(0)\n",
    "            videos.append(video)\n",
    "            labels.append(torch.tensor(label, dtype=torch.long))\n",
    "        \n",
    "        videos = torch.stack(videos)\n",
    "        labels = torch.stack(labels)\n",
    "        return videos, labels\n",
    "    \n",
    "    if subset_ratio is not None:\n",
    "        num_samples = int(len(dataset) * subset_ratio) \n",
    "        subset_indices = list(range(num_samples))\n",
    "        subset = Subset(dataset, subset_indices)\n",
    "        dataloader = DataLoader(subset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    else:\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb8ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_dataloader, val_dataloader = None, \n",
    "                  epochs=5, learning_rate=2e-4, early_stopping_patience=10,\n",
    "                  checkpoint_path='best_model_VideoMAE.pth', lr_patience=2):\n",
    "\n",
    "    results_dict = {\n",
    "        'time_per_epoch': [],\n",
    "        'train_accuracy': [],\n",
    "        'train_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_loss': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'learning_rate': [],\n",
    "    }\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # ReduceLROnPlateau - tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=lr_patience, min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    # EarlyStopping - tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    epoch_pbar = tqdm(range(epochs), desc=\"Training Epochs\")\n",
    "\n",
    "    for epoch in epoch_pbar:\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        epoch_start = time()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for videos, labels in train_dataloader:\n",
    "            videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            all_preds.extend(outputs.argmax(dim=1).cpu().detach().numpy())\n",
    "            all_labels.extend(labels.cpu().detach().numpy())\n",
    "        \n",
    "        epoch_time = time() - epoch_start\n",
    "        results_dict['time_per_epoch'].append(epoch_time)\n",
    "        \n",
    "        # MetricsCallback - tracks precision, recall, f1\n",
    "        results_dict['train_accuracy'].append(accuracy_score(all_labels, all_preds))\n",
    "        results_dict['precision'].append(precision_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
    "        results_dict['recall'].append(recall_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
    "        results_dict['f1'].append(f1_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_dataloader)\n",
    "        results_dict['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            val_accuracy, val_loss = infer_loop(model, val_dataloader, criterion)\n",
    "        \n",
    "            results_dict['val_accuracy'].append(val_accuracy)\n",
    "            results_dict['val_loss'].append(val_loss)\n",
    "            results_dict['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            # ReduceLROnPlateau callback\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # ModelCheckpoint - tf.keras.callbacks.ModelCheckpoint('best_model_TVN.h5', monitor='val_loss', save_best_only=True)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_without_improvement = 0\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                epoch_pbar.write(f\"✓ Epoch {epoch+1}: Saved best model with val_loss: {val_loss:.4f}\")\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            epoch_pbar.set_postfix({\n",
    "                'train_loss': f\"{avg_train_loss:.4f}\",\n",
    "                'val_loss': f\"{val_loss:.4f}\",\n",
    "                'val_acc': f\"{val_accuracy:.4f}\",\n",
    "                'lr': f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
    "            })\n",
    "        else:\n",
    "            # Update progress bar without validation\n",
    "            epoch_pbar.set_postfix({\n",
    "                'val_loss' : \"Not Given\",\n",
    "                'train_loss': f\"{avg_train_loss:.4f}\",\n",
    "                'lr': f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
    "            })        \n",
    "        # EarlyStopping check\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            epoch_pbar.write(f\"\\n⚠ Early stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddbadb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_loop(model, dataloader, criterion = None) -> tuple:\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    for videos, labels in dataloader:\n",
    "        videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        all_preds.extend(outputs.argmax(dim=1).cpu().detach().numpy())\n",
    "        all_labels.extend(labels.cpu().detach().numpy())\n",
    "        \n",
    "    val_accuracy = accuracy_score(all_preds, all_labels)\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "\n",
    "    return (val_accuracy, avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35eb1301",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader =  get_dataloader(train_dataset, subset_ratio=0.1, batch_size=16)\n",
    "val_dataloader =  get_dataloader(val_dataset, subset_ratio=0.1, batch_size=16)\n",
    "test_dataloader =  get_dataloader(test_dataset, subset_ratio=None, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8577c87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  20%|██        | 1/5 [00:15<01:00, 15.24s/it, train_loss=2.1107, val_loss=2.4723, val_acc=0.0000, lr=2.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Epoch 1: Saved best model with val_loss: 2.4723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  40%|████      | 2/5 [00:27<00:40, 13.58s/it, train_loss=2.1546, val_loss=2.4260, val_acc=0.0000, lr=2.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Epoch 2: Saved best model with val_loss: 2.4260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  60%|██████    | 3/5 [00:40<00:26, 13.10s/it, train_loss=2.0770, val_loss=2.3822, val_acc=0.0000, lr=2.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Epoch 3: Saved best model with val_loss: 2.3822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  80%|████████  | 4/5 [00:52<00:12, 12.92s/it, train_loss=2.1402, val_loss=2.3460, val_acc=0.0000, lr=2.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Epoch 4: Saved best model with val_loss: 2.3460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 5/5 [01:05<00:00, 13.01s/it, train_loss=2.1253, val_loss=2.3057, val_acc=0.0000, lr=2.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Epoch 5: Saved best model with val_loss: 2.3057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = training_loop(\n",
    "    MODEL, \n",
    "    train_dataloader, val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29120230",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './models/trained_models/VideoMAE_Trained.pth'\n",
    "torch.save(MODEL.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3df96266",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(results, orient=\"index\").T\n",
    "results_df.to_csv(\"VideoMAE_Results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fae5c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_per_batch</th>\n",
       "      <th>time_per_epoch</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>learning_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.044201</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.230697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.433681</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.254749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.044138</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.365645</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.132989</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.044138</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.248171</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.123748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.044138</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.534895</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.102612</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.044138</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_per_batch  time_per_epoch  train_accuracy  train_loss  val_accuracy  \\\n",
       "0             NaN       10.044201            0.04    2.230697           NaN   \n",
       "1             NaN        9.433681            0.16    2.254749           NaN   \n",
       "2             NaN        9.365645            0.16    2.132989           NaN   \n",
       "3             NaN        9.248171            0.16    2.123748           NaN   \n",
       "4             NaN        9.534895            0.16    2.102612           NaN   \n",
       "\n",
       "   val_loss  precision  recall        f1  learning_rate  \n",
       "0       NaN     0.0016    0.04  0.003077            NaN  \n",
       "1       NaN     0.0256    0.16  0.044138            NaN  \n",
       "2       NaN     0.0256    0.16  0.044138            NaN  \n",
       "3       NaN     0.0256    0.16  0.044138            NaN  \n",
       "4       NaN     0.0256    0.16  0.044138            NaN  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b383a55b",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL.load_state_dict(torch.load(\"models\\\\trained_models\\\\VideoMAE_Trained.pth\", map_location=DEVICE))\n",
    "MODEL.to(DEVICE)\n",
    "MODEL.eval()\n",
    "test_acc, test_loss = infer_loop(MODEL, test_dataloader)\n",
    "test_acc, test_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geture-research-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
