{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b348d8c",
   "metadata": {},
   "source": [
    "# Initial Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1273de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from colorVideoDataset import ColorVideoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49168ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = ColorVideoDataset('./colors', sequence_length=16)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd470b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = random_split(DATASET, [int(0.8 * len(DATASET)), len(DATASET) - int(0.8 * len(DATASET))])\n",
    "test_dataset, val_dataset = random_split(test_dataset, [int(0.5 * len(test_dataset)), len(test_dataset) - int(0.5 * len(test_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7824f7d8",
   "metadata": {},
   "source": [
    "# PyTorch 3D ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ee457ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from models.ThreeDResNet import get_3dResNet, get_resnet_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dbb3203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Arnav Waghdhare/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (blocks): ModuleList(\n",
      "    (0): ResNetBasicStem(\n",
      "      (conv): Conv3d(3, 64, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False)\n",
      "      (norm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "      (pool): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=[0, 1, 1], dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): ResStage(\n",
      "      (res_blocks): ModuleList(\n",
      "        (0): ResBlock(\n",
      "          (branch1_conv): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(np.int64(1), np.int64(1), np.int64(1)), bias=False)\n",
      "          (branch1_norm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_a): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "        (1-2): 2 x ResBlock(\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_a): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ResStage(\n",
      "      (res_blocks): ModuleList(\n",
      "        (0): ResBlock(\n",
      "          (branch1_conv): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(np.int64(1), np.int64(2), np.int64(2)), bias=False)\n",
      "          (branch1_norm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_a): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "        (1-3): 3 x ResBlock(\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_a): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): ResStage(\n",
      "      (res_blocks): ModuleList(\n",
      "        (0): ResBlock(\n",
      "          (branch1_conv): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(np.int64(1), np.int64(2), np.int64(2)), bias=False)\n",
      "          (branch1_norm): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(512, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "            (norm_a): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "        (1-5): 5 x ResBlock(\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(1024, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "            (norm_a): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): ResStage(\n",
      "      (res_blocks): ModuleList(\n",
      "        (0): ResBlock(\n",
      "          (branch1_conv): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(np.int64(1), np.int64(2), np.int64(2)), bias=False)\n",
      "          (branch1_norm): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(1024, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "            (norm_a): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(512, 512, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "        (1-2): 2 x ResBlock(\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(2048, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "            (norm_a): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(512, 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): ResNetBasicHead(\n",
      "      (pool): AvgPool3d(kernel_size=(8, 7, 7), stride=(1, 1, 1), padding=(0, 0, 0))\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (proj): Linear(in_features=2048, out_features=8, bias=True)\n",
      "      (output_pool): AdaptiveAvgPool3d(output_size=1)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "MODEL = get_3dResNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097d56fe",
   "metadata": {},
   "source": [
    "Train, Test, Val Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e3a0cc",
   "metadata": {},
   "source": [
    "## Dataloader and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f516fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, subset_ratio : float | None = 0.1, batch_size : int = 2):\n",
    "    transform = get_resnet_transformer()\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        videos = []\n",
    "        labels = []\n",
    "        for video, label, _ in batch:\n",
    "            video = video.permute(1, 0, 2, 3)\n",
    "            video = transform({\"video\": video})[\"video\"]\n",
    "            videos.append(video)\n",
    "            labels.append(torch.tensor(label, dtype=torch.long))\n",
    "        \n",
    "        videos = torch.stack(videos)\n",
    "        labels = torch.stack(labels)\n",
    "        return videos, labels\n",
    "    \n",
    "    if subset_ratio is not None:\n",
    "        num_samples = int(len(dataset) * subset_ratio) \n",
    "        subset_indices = list(range(num_samples))\n",
    "        subset = Subset(dataset, subset_indices)\n",
    "        dataloader = DataLoader(subset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    else:\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b58341e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_loop(model, dataloader, criterion = None) -> tuple:\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    for videos, labels in dataloader:\n",
    "        videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        all_preds.extend(outputs.argmax(dim=1).cpu().detach().numpy())\n",
    "        all_labels.extend(labels.cpu().detach().numpy())\n",
    "        \n",
    "    val_accuracy = accuracy_score(all_preds, all_labels)\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "\n",
    "    return (val_accuracy, avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_dataloader, val_dataloader: DataLoader | None = None, \n",
    "                  epochs=5, learning_rate=2e-4, early_stopping_patience=10,\n",
    "                  checkpoint_path='best_model_3DResNet.pth', lr_patience=2):\n",
    "\n",
    "    results_dict = {\n",
    "        'time_per_batch': [],\n",
    "        'time_per_epoch': [],\n",
    "        'train_accuracy': [],\n",
    "        'train_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_loss': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'learning_rate': [],\n",
    "    }\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # ReduceLROnPlateau - tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=lr_patience, verbose=True, min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    # EarlyStopping - tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    epoch_pbar = tqdm(range(epochs), desc=\"Training Epochs\")\n",
    "\n",
    "    for epoch in epoch_pbar:\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        epoch_start = time()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for videos, labels in train_dataloader:\n",
    "            videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            all_preds.extend(outputs.argmax(dim=1).cpu().detach().numpy())\n",
    "            all_labels.extend(labels.cpu().detach().numpy())\n",
    "        \n",
    "        epoch_time = time() - epoch_start\n",
    "        results_dict['time_per_epoch'].append(epoch_time)\n",
    "        \n",
    "        # MetricsCallback - tracks precision, recall, f1\n",
    "        results_dict['train_accuracy'].append(accuracy_score(all_labels, all_preds))\n",
    "        results_dict['precision'].append(precision_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
    "        results_dict['recall'].append(recall_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
    "        results_dict['f1'].append(f1_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_dataloader)\n",
    "        results_dict['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        val_accuracy, val_loss = infer_loop(model, val_dataloader, criterion)\n",
    "        \n",
    "        results_dict['val_accuracy'].append(val_accuracy)\n",
    "        results_dict['val_loss'].append(val_loss)\n",
    "        results_dict['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # ReduceLROnPlateau callback\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # ModelCheckpoint - tf.keras.callbacks.ModelCheckpoint('best_model_TVN.h5', monitor='val_loss', save_best_only=True)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            epoch_pbar.write(f\"✓ Epoch {epoch+1}: Saved best model with val_loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        epoch_pbar.set_postfix({\n",
    "            'train_loss': f\"{avg_train_loss:.4f}\",\n",
    "            'val_loss': f\"{val_loss:.4f}\",\n",
    "            'val_acc': f\"{val_accuracy:.4f}\",\n",
    "            'lr': f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
    "        })\n",
    "        \n",
    "        # EarlyStopping check\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            epoch_pbar.write(f\"\\n⚠ Early stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    epoch_pbar.write(f\"\\n✓ Loaded best model from {checkpoint_path}\")\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = get_dataloader(DATASET, subset_ratio=0.1, batch_size=16)\n",
    "x, y = next(iter(dataloader))\n",
    "print(x.shape, y.shape)\n",
    "del x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95223ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader =  get_dataloader(train_dataset, subset_ratio=None, batch_size=16)\n",
    "val_dataloader =  get_dataloader(val_dataset, subset_ratio=None, batch_size=16)\n",
    "test_dataloader =  get_dataloader(test_dataset, subset_ratio=None, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a57a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = training_loop(\n",
    "    MODEL, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    epochs=100, \n",
    "    learning_rate=2e-4,\n",
    "    early_stopping_patience=10,              \n",
    "    checkpoint_path='best_model_3DResNet.pth',  \n",
    "    lr_patience=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe37e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './models/trained/3dResnet_Trained.pth'\n",
    "torch.save(MODEL.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(results, orient=\"index\").T\n",
    "results_df.to_csv(\"3D_ResNet_Results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b7b57",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1dc20aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9375, 0.259842649102211)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL.load_state_dict(torch.load(\"models\\\\trained_models\\\\3dResnet_Trained.pth\", map_location=DEVICE))\n",
    "MODEL.to(DEVICE)\n",
    "MODEL.eval()\n",
    "test_acc, test_loss = infer_loop(MODEL, test_dataloader)\n",
    "test_acc, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc92cc",
   "metadata": {},
   "source": [
    "# TensorFlow TinyVideoNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87e7c33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow_hub\\__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\models\\TinyVideoNet.py:7: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as k\n",
    "import numpy as np\n",
    "from models.TinyVideoNet import TinyVideoNetTransfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41af0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_data_generator(torch_dataset):\n",
    "    def generator():\n",
    "        for i in range(len(torch_dataset)):\n",
    "            video, label, _ = torch_dataset[i]\n",
    "            yield video, label\n",
    "    return generator\n",
    "\n",
    "def create_dataset(torch_ds, batch_size=4):\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        tf_data_generator(torch_ds),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(30, 3, 480, 640), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "        )\n",
    "    )\n",
    "    return ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ceb74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF Datasets\n",
    "tf_train = create_dataset(train_dataset, batch_size=4)\n",
    "tf_val = create_dataset(val_dataset, batch_size=4)\n",
    "tf_test = create_dataset(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3130be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(x, y, num_classes=8):\n",
    "    y_one_hot = tf.one_hot(y, depth=num_classes)\n",
    "    return x, y_one_hot\n",
    "\n",
    "tf_train = tf_train.map(lambda x, y: to_one_hot(x, y, 8))\n",
    "tf_val = tf_val.map(lambda x, y: to_one_hot(x, y, 8))\n",
    "tf_test = tf_test.map(lambda x, y: to_one_hot(x, y, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2618f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_handle = 'https://kaggle.com/models/google/tiny-video-net/frameworks/TensorFlow1/variations/tvn1/versions/1'\n",
    "tf_model = TinyVideoNetTransfer(model_handle, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed7f3988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCallback(k.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.time_per_epoch = []  # Track epoch times\n",
    "        self.epoch_start_time = None\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Calculate epoch time\n",
    "        epoch_time = time() - self.epoch_start_time\n",
    "        self.time_per_epoch.append(epoch_time)\n",
    "        logs['time_per_epoch'] = epoch_time\n",
    "\n",
    "# Create the callback\n",
    "metrics_callback = MetricsCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28483f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.compile(\n",
    "    optimizer=k.optimizers.Adam(learning_rate=3e-4), \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        k.metrics.Precision(),\n",
    "        k.metrics.Recall(), \n",
    "        k.metrics.F1Score(average='macro')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc9cbb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae00aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (4, 30, 3, 480, 640)\n",
      "y.shape: (4, 8)\n",
      "y.dtype: <dtype: 'float32'>\n",
      "Sample label: [0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(tf_train))\n",
    "print(\"x.shape:\", x.shape)  # Should be (B, 30, 3, 480, 640)\n",
    "print(\"y.shape:\", y.shape)  # Should be (B,)\n",
    "print(\"y.dtype:\", y.dtype)  # Should be int32 or int64\n",
    "print(\"Sample label:\", y[0].numpy())  # Should be integer in [0, 7]\n",
    "\n",
    "del x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa8c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "      8/Unknown \u001b[1m8s\u001b[0m 859ms/step - accuracy: 0.3141 - f1_score: 0.1696 - loss: 2.4431 - precision: 0.1079 - recall: 0.0381   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:164: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - accuracy: 0.2812 - f1_score: 0.1873 - loss: 2.2561 - precision: 0.1250 - recall: 0.0312 - val_accuracy: 0.1875 - val_f1_score: 0.0691 - val_loss: 2.1706 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04 - time_per_epoch: 15.1407\n",
      "Epoch 2/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.2500 - f1_score: 0.1849 - loss: 2.4227 - precision: 0.2857 - recall: 0.0625 - val_accuracy: 0.0938 - val_f1_score: 0.0460 - val_loss: 2.1545 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04 - time_per_epoch: 13.6725\n",
      "Epoch 3/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.1875 - f1_score: 0.1184 - loss: 2.1613 - precision: 0.2222 - recall: 0.0625 - val_accuracy: 0.0938 - val_f1_score: 0.0420 - val_loss: 2.1684 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04 - time_per_epoch: 14.0636\n",
      "Epoch 4/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.1562 - f1_score: 0.0994 - loss: 2.2509 - precision: 0.4286 - recall: 0.0938 - val_accuracy: 0.0938 - val_f1_score: 0.0455 - val_loss: 2.1158 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04 - time_per_epoch: 13.2234\n",
      "Epoch 5/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.2500 - f1_score: 0.1601 - loss: 2.0161 - precision: 0.3333 - recall: 0.0938 - val_accuracy: 0.0938 - val_f1_score: 0.0442 - val_loss: 2.2150 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04 - time_per_epoch: 13.0330\n"
     ]
    }
   ],
   "source": [
    "history = tf_model.fit(\n",
    "    tf_train,\n",
    "    validation_data=tf_val, \n",
    "    epochs=5, \n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10), \n",
    "        tf.keras.callbacks.ModelCheckpoint('./models/trained_models/TVN_best_model.weights.h5', monitor='val_loss', save_best_only=True, save_weights_only=True), \n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.1, min_lr=1e-7), \n",
    "        metrics_callback\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40c068f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.28125, 0.25, 0.1875, 0.15625, 0.25],\n",
       " 'f1_score': [0.18733972311019897,\n",
       "  0.18492060899734497,\n",
       "  0.11837119609117508,\n",
       "  0.09940474480390549,\n",
       "  0.16011902689933777],\n",
       " 'loss': [2.256065845489502,\n",
       "  2.422661066055298,\n",
       "  2.161315441131592,\n",
       "  2.2509493827819824,\n",
       "  2.0161006450653076],\n",
       " 'precision': [0.125,\n",
       "  0.2857142984867096,\n",
       "  0.2222222238779068,\n",
       "  0.4285714328289032,\n",
       "  0.3333333432674408],\n",
       " 'recall': [0.03125, 0.0625, 0.0625, 0.09375, 0.09375],\n",
       " 'val_accuracy': [0.1875, 0.09375, 0.09375, 0.09375, 0.09375],\n",
       " 'val_f1_score': [0.06905370205640793,\n",
       "  0.045955877751111984,\n",
       "  0.041958037763834,\n",
       "  0.045454539358615875,\n",
       "  0.04417292773723602],\n",
       " 'val_loss': [2.170637845993042,\n",
       "  2.1545023918151855,\n",
       "  2.168384075164795,\n",
       "  2.115834951400757,\n",
       "  2.214994192123413],\n",
       " 'val_precision': [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'val_recall': [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'learning_rate': [0.00019999999494757503,\n",
       "  0.00019999999494757503,\n",
       "  0.00019999999494757503,\n",
       "  0.00019999999494757503,\n",
       "  0.00019999999494757503],\n",
       " 'time_per_epoch': [15.140650987625122,\n",
       "  13.672520875930786,\n",
       "  14.063580513000488,\n",
       "  13.223387479782104,\n",
       "  13.03298544883728]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfff106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(history.history, orient=\"index\").T\n",
    "results_df.to_csv(\"TVN_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86d25261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>loss</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_f1_score</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>time_per_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.28125</td>\n",
       "      <td>0.187340</td>\n",
       "      <td>2.256066</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>0.18750</td>\n",
       "      <td>0.069054</td>\n",
       "      <td>2.170638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>15.140651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.184921</td>\n",
       "      <td>2.422661</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>0.09375</td>\n",
       "      <td>0.045956</td>\n",
       "      <td>2.154502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>13.672521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.18750</td>\n",
       "      <td>0.118371</td>\n",
       "      <td>2.161315</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>0.09375</td>\n",
       "      <td>0.041958</td>\n",
       "      <td>2.168384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>14.063581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.15625</td>\n",
       "      <td>0.099405</td>\n",
       "      <td>2.250949</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.09375</td>\n",
       "      <td>0.09375</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>2.115835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>13.223387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.160119</td>\n",
       "      <td>2.016101</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.09375</td>\n",
       "      <td>0.09375</td>\n",
       "      <td>0.044173</td>\n",
       "      <td>2.214994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>13.032985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  f1_score      loss  precision   recall  val_accuracy  \\\n",
       "0   0.28125  0.187340  2.256066   0.125000  0.03125       0.18750   \n",
       "1   0.25000  0.184921  2.422661   0.285714  0.06250       0.09375   \n",
       "2   0.18750  0.118371  2.161315   0.222222  0.06250       0.09375   \n",
       "3   0.15625  0.099405  2.250949   0.428571  0.09375       0.09375   \n",
       "4   0.25000  0.160119  2.016101   0.333333  0.09375       0.09375   \n",
       "\n",
       "   val_f1_score  val_loss  val_precision  val_recall  learning_rate  \\\n",
       "0      0.069054  2.170638            0.0         0.0         0.0002   \n",
       "1      0.045956  2.154502            0.0         0.0         0.0002   \n",
       "2      0.041958  2.168384            0.0         0.0         0.0002   \n",
       "3      0.045455  2.115835            0.0         0.0         0.0002   \n",
       "4      0.044173  2.214994            0.0         0.0         0.0002   \n",
       "\n",
       "   time_per_epoch  \n",
       "0       15.140651  \n",
       "1       13.672521  \n",
       "2       14.063581  \n",
       "3       13.223387  \n",
       "4       13.032985  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5409a4cf",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e547d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.build(input_shape=(None, 16, 3, 480, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e136167f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nTypeError: `generator` yielded an element of shape (16, 3, 480, 640) where an element of shape (30, 3, 480, 640) was expected.\nTraceback (most recent call last):\n\n  File \"c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 235, in generator_py_func\n    raise TypeError(\n\nTypeError: `generator` yielded an element of shape (16, 3, 480, 640) where an element of shape (30, 3, 480, 640) was expected.\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_4190]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m tf_model.load_weights(\u001b[33m'\u001b[39m\u001b[33mmodels/trained_models/TVN_best_model.weights.h5\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Evaluate the model on the test dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m test_loss, test_accuracy = \u001b[43mtf_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nTypeError: `generator` yielded an element of shape (16, 3, 480, 640) where an element of shape (30, 3, 480, 640) was expected.\nTraceback (most recent call last):\n\n  File \"c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 235, in generator_py_func\n    raise TypeError(\n\nTypeError: `generator` yielded an element of shape (16, 3, 480, 640) where an element of shape (30, 3, 480, 640) was expected.\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_4190]"
     ]
    }
   ],
   "source": [
    "# Load the best model weights\n",
    "tf_model.load_weights('models/trained_models/TVN_best_model.weights.h5')\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = tf_model.evaluate(tf_test)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc733eeb",
   "metadata": {},
   "source": [
    "# PyTorch VideoMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19870247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models.VideoMAE_SL import VideoMAEClassifier\n",
    "from transformers import VideoMAEImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab638c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 184/184 [00:00<00:00, 989.18it/s, Materializing param=layernorm.weight]                                 \n",
      "VideoMAEModel LOAD REPORT from: MCG-NJU/videomae-base\n",
      "Key                                                                  | Status     |  | \n",
      "---------------------------------------------------------------------+------------+--+-\n",
      "decoder.norm.bias                                                    | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.attention.output.dense.weight    | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.attention.output.dense.bias      | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.layernorm_before.weight          | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.attention.attention.v_bias       | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.output.dense.bias                | UNEXPECTED |  | \n",
      "decoder.head.bias                                                    | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.intermediate.dense.bias          | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.output.dense.weight              | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.attention.attention.key.weight   | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.layernorm_after.weight           | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.layernorm_after.bias             | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.attention.attention.value.weight | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.attention.attention.query.weight | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.attention.attention.q_bias       | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.intermediate.dense.weight        | UNEXPECTED |  | \n",
      "decoder.decoder_layers.{0, 1, 2, 3}.layernorm_before.bias            | UNEXPECTED |  | \n",
      "decoder.norm.weight                                                  | UNEXPECTED |  | \n",
      "mask_token                                                           | UNEXPECTED |  | \n",
      "encoder_to_decoder.weight                                            | UNEXPECTED |  | \n",
      "decoder.head.weight                                                  | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "MODEL = VideoMAEClassifier(num_classes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cdb415",
   "metadata": {},
   "source": [
    "## Dataloader and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4876d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, subset_ratio : float | None = 0.1, batch_size : int = 2):\n",
    "    processor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        videos = []\n",
    "        labels = []\n",
    "        for video, label, _ in batch:\n",
    "            video = video.permute(0, 2, 3, 1)\n",
    "            video_list = [video[i].numpy() for i in range(video.shape[0])]\n",
    "\n",
    "            video = processor(video_list, return_tensors=\"pt\", do_normalize=False, do_convert_rgb=False)[\"pixel_values\"].squeeze(0) \n",
    "            # video = processor(video, return_tensors=\"pt\", do_normalize=False, do_convert_rgb=False, do_center_crop=False)[\"pixel_values\"].squeeze(0)\n",
    "            videos.append(video)\n",
    "            labels.append(torch.tensor(label, dtype=torch.long))\n",
    "        \n",
    "        videos = torch.stack(videos)\n",
    "        labels = torch.stack(labels)\n",
    "        return videos, labels\n",
    "    \n",
    "    if subset_ratio is not None:\n",
    "        num_samples = int(len(dataset) * subset_ratio) \n",
    "        subset_indices = list(range(num_samples))\n",
    "        subset = Subset(dataset, subset_indices)\n",
    "        dataloader = DataLoader(subset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    else:\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_dataloader, val_dataloader = None, \n",
    "                  epochs=5, learning_rate=2e-4, early_stopping_patience=10,\n",
    "                  checkpoint_path='best_model_VideoMAE.pth', lr_patience=2):\n",
    "\n",
    "    results_dict = {\n",
    "        'time_per_epoch': [],\n",
    "        'train_accuracy': [],\n",
    "        'train_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_loss': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'learning_rate': [],\n",
    "    }\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # ReduceLROnPlateau - tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=lr_patience, min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    # EarlyStopping - tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    epoch_pbar = tqdm(range(epochs), desc=\"Training Epochs\")\n",
    "\n",
    "    for epoch in epoch_pbar:\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        epoch_start = time()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for videos, labels in train_dataloader:\n",
    "            videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            all_preds.extend(outputs.argmax(dim=1).cpu().detach().numpy())\n",
    "            all_labels.extend(labels.cpu().detach().numpy())\n",
    "        \n",
    "        epoch_time = time() - epoch_start\n",
    "        results_dict['time_per_epoch'].append(epoch_time)\n",
    "        \n",
    "        # MetricsCallback - tracks precision, recall, f1\n",
    "        results_dict['train_accuracy'].append(accuracy_score(all_labels, all_preds))\n",
    "        results_dict['precision'].append(precision_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
    "        results_dict['recall'].append(recall_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
    "        results_dict['f1'].append(f1_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_dataloader)\n",
    "        results_dict['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            val_accuracy, val_loss = infer_loop(model, val_dataloader, criterion)\n",
    "        \n",
    "            results_dict['val_accuracy'].append(val_accuracy)\n",
    "            results_dict['val_loss'].append(val_loss)\n",
    "            results_dict['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            # ReduceLROnPlateau callback\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # ModelCheckpoint - tf.keras.callbacks.ModelCheckpoint('best_model_TVN.h5', monitor='val_loss', save_best_only=True)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_without_improvement = 0\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                epoch_pbar.write(f\"✓ Epoch {epoch+1}: Saved best model with val_loss: {val_loss:.4f}\")\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            epoch_pbar.set_postfix({\n",
    "                'train_loss': f\"{avg_train_loss:.4f}\",\n",
    "                'val_loss': f\"{val_loss:.4f}\",\n",
    "                'val_acc': f\"{val_accuracy:.4f}\",\n",
    "                'lr': f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
    "            })\n",
    "        else:\n",
    "            # Update progress bar without validation\n",
    "            epoch_pbar.set_postfix({\n",
    "                'val_loss' : \"Not Given\",\n",
    "                'train_loss': f\"{avg_train_loss:.4f}\",\n",
    "                'lr': f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
    "            })        \n",
    "        # EarlyStopping check\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            epoch_pbar.write(f\"\\n⚠ Early stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddbadb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_loop(model, dataloader, criterion = None) -> tuple:\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    for videos, labels in dataloader:\n",
    "        videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        all_preds.extend(outputs.argmax(dim=1).cpu().detach().numpy())\n",
    "        all_labels.extend(labels.cpu().detach().numpy())\n",
    "        \n",
    "    val_accuracy = accuracy_score(all_preds, all_labels)\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "\n",
    "    return (val_accuracy, avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35eb1301",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader =  get_dataloader(train_dataset, subset_ratio=0.1, batch_size=16)\n",
    "val_dataloader =  get_dataloader(val_dataset, subset_ratio=None, batch_size=16)\n",
    "test_dataloader =  get_dataloader(test_dataset, subset_ratio=None, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8577c87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\transformers\\image_processing_utils.py:50: UserWarning: The following named arguments are not valid for `VideoMAEImageProcessor.preprocess` and were ignored: 'do_convert_rgb'\n",
      "  return self.preprocess(images, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Shape in Collate_fn: torch.Size([16, 16, 3, 224, 224])\n",
      "Video Shape in Collate_fn: torch.Size([9, 16, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  20%|██        | 1/5 [00:10<00:40, 10.06s/it, val_loss=Not Given, train_loss=2.2307, lr=2.00e-04]c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\transformers\\image_processing_utils.py:50: UserWarning: The following named arguments are not valid for `VideoMAEImageProcessor.preprocess` and were ignored: 'do_convert_rgb'\n",
      "  return self.preprocess(images, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Shape in Collate_fn: torch.Size([16, 16, 3, 224, 224])\n",
      "Video Shape in Collate_fn: torch.Size([9, 16, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  40%|████      | 2/5 [00:19<00:29,  9.69s/it, val_loss=Not Given, train_loss=2.2547, lr=2.00e-04]c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\transformers\\image_processing_utils.py:50: UserWarning: The following named arguments are not valid for `VideoMAEImageProcessor.preprocess` and were ignored: 'do_convert_rgb'\n",
      "  return self.preprocess(images, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Shape in Collate_fn: torch.Size([16, 16, 3, 224, 224])\n",
      "Video Shape in Collate_fn: torch.Size([9, 16, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  60%|██████    | 3/5 [00:28<00:19,  9.55s/it, val_loss=Not Given, train_loss=2.1330, lr=2.00e-04]c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\transformers\\image_processing_utils.py:50: UserWarning: The following named arguments are not valid for `VideoMAEImageProcessor.preprocess` and were ignored: 'do_convert_rgb'\n",
      "  return self.preprocess(images, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Shape in Collate_fn: torch.Size([16, 16, 3, 224, 224])\n",
      "Video Shape in Collate_fn: torch.Size([9, 16, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  80%|████████  | 4/5 [00:38<00:09,  9.43s/it, val_loss=Not Given, train_loss=2.1237, lr=2.00e-04]c:\\Users\\Arnav Waghdhare\\Desktop\\Arnav20\\Coding\\Python\\geture_research_ml\\.venv\\Lib\\site-packages\\transformers\\image_processing_utils.py:50: UserWarning: The following named arguments are not valid for `VideoMAEImageProcessor.preprocess` and were ignored: 'do_convert_rgb'\n",
      "  return self.preprocess(images, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Shape in Collate_fn: torch.Size([16, 16, 3, 224, 224])\n",
      "Video Shape in Collate_fn: torch.Size([9, 16, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 5/5 [00:47<00:00,  9.53s/it, val_loss=Not Given, train_loss=2.1026, lr=2.00e-04]\n"
     ]
    }
   ],
   "source": [
    "results = training_loop(\n",
    "    MODEL, \n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    epochs=100, \n",
    "    learning_rate=2e-4,\n",
    "    early_stopping_patience=10,              \n",
    "    checkpoint_path='best_model_3DResNet.pth',  \n",
    "    lr_patience=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29120230",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './models/trained_models/VideoMAE_Trained.pth'\n",
    "torch.save(MODEL.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3df96266",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(results, orient=\"index\").T\n",
    "results_df.to_csv(\"VideoMAE_Results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fae5c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_per_batch</th>\n",
       "      <th>time_per_epoch</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>learning_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.044201</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.230697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.433681</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.254749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.044138</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.365645</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.132989</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.044138</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.248171</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.123748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.044138</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.534895</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.102612</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.044138</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_per_batch  time_per_epoch  train_accuracy  train_loss  val_accuracy  \\\n",
       "0             NaN       10.044201            0.04    2.230697           NaN   \n",
       "1             NaN        9.433681            0.16    2.254749           NaN   \n",
       "2             NaN        9.365645            0.16    2.132989           NaN   \n",
       "3             NaN        9.248171            0.16    2.123748           NaN   \n",
       "4             NaN        9.534895            0.16    2.102612           NaN   \n",
       "\n",
       "   val_loss  precision  recall        f1  learning_rate  \n",
       "0       NaN     0.0016    0.04  0.003077            NaN  \n",
       "1       NaN     0.0256    0.16  0.044138            NaN  \n",
       "2       NaN     0.0256    0.16  0.044138            NaN  \n",
       "3       NaN     0.0256    0.16  0.044138            NaN  \n",
       "4       NaN     0.0256    0.16  0.044138            NaN  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b383a55b",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL.load_state_dict(torch.load(\"models\\\\trained_models\\\\VideoMAE_Trained.pth\", map_location=DEVICE))\n",
    "MODEL.to(DEVICE)\n",
    "MODEL.eval()\n",
    "test_acc, test_loss = infer_loop(MODEL, test_dataloader)\n",
    "test_acc, test_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geture-research-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
