{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7824f7d8",
   "metadata": {},
   "source": [
    "# PyTorch 3D ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee457ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\ConvLSTMPyTorchLightning\\.venv\\Lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\ConvLSTMPyTorchLightning\\.venv\\Lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score, r2_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "\n",
    "from models.ThreeDResNet import get_3dResNet, get_resnet_transformer\n",
    "from colorVideoDataset import ColorVideoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49168ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (blocks): ModuleList(\n",
      "    (0): ResNetBasicStem(\n",
      "      (conv): Conv3d(3, 64, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False)\n",
      "      (norm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "      (pool): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=[0, 1, 1], dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): ResStage(\n",
      "      (res_blocks): ModuleList(\n",
      "        (0): ResBlock(\n",
      "          (branch1_conv): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(np.int64(1), np.int64(1), np.int64(1)), bias=False)\n",
      "          (branch1_norm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_a): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "        (1-2): 2 x ResBlock(\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_a): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ResStage(\n",
      "      (res_blocks): ModuleList(\n",
      "        (0): ResBlock(\n",
      "          (branch1_conv): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(np.int64(1), np.int64(2), np.int64(2)), bias=False)\n",
      "          (branch1_norm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_a): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "        (1-3): 3 x ResBlock(\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_a): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): ResStage(\n",
      "      (res_blocks): ModuleList(\n",
      "        (0): ResBlock(\n",
      "          (branch1_conv): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(np.int64(1), np.int64(2), np.int64(2)), bias=False)\n",
      "          (branch1_norm): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(512, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "            (norm_a): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "        (1-5): 5 x ResBlock(\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(1024, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "            (norm_a): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): ResStage(\n",
      "      (res_blocks): ModuleList(\n",
      "        (0): ResBlock(\n",
      "          (branch1_conv): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(np.int64(1), np.int64(2), np.int64(2)), bias=False)\n",
      "          (branch1_norm): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(1024, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "            (norm_a): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(512, 512, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "        (1-2): 2 x ResBlock(\n",
      "          (branch2): BottleneckBlock(\n",
      "            (conv_a): Conv3d(2048, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "            (norm_a): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_a): ReLU()\n",
      "            (conv_b): Conv3d(512, 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "            (norm_b): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act_b): ReLU()\n",
      "            (conv_c): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (norm_c): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): ResNetBasicHead(\n",
      "      (pool): AvgPool3d(kernel_size=(8, 7, 7), stride=(1, 1, 1), padding=(0, 0, 0))\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (proj): Linear(in_features=2048, out_features=8, bias=True)\n",
      "      (output_pool): AdaptiveAvgPool3d(output_size=1)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "MODEL = get_3dResNet()\n",
    "DATASET = ColorVideoDataset('./colors')\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097d56fe",
   "metadata": {},
   "source": [
    "Train, Test, Val Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd470b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = random_split(DATASET, [int(0.8 * len(DATASET)), len(DATASET) - int(0.8 * len(DATASET))])\n",
    "test_dataset, val_dataset = random_split(test_dataset, [int(0.5 * len(test_dataset)), len(test_dataset) - int(0.5 * len(test_dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f516fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, subset_ratio : float | None = 0.1, batch_size : int = 2):\n",
    "    transform = get_resnet_transformer()\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        videos = []\n",
    "        labels = []\n",
    "        for video, label, _ in batch:\n",
    "            video = video.permute(1, 0, 2, 3)\n",
    "            video = transform({\"video\": video})[\"video\"]\n",
    "            videos.append(video)\n",
    "            labels.append(torch.tensor(label, dtype=torch.long))\n",
    "        \n",
    "        videos = torch.stack(videos)\n",
    "        labels = torch.stack(labels)\n",
    "        return videos, labels\n",
    "    \n",
    "    if subset_ratio is not None:\n",
    "        num_samples = int(len(dataset) * subset_ratio) \n",
    "        subset_indices = list(range(num_samples))\n",
    "        subset = Subset(dataset, subset_indices)\n",
    "        dataloader = DataLoader(subset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    else:\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b58341e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_loop(model, dataloader, criterion) -> tuple:\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    for videos, labels in dataloader:\n",
    "        videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        all_preds.extend(outputs.argmax(dim=1).cpu().detach().numpy())\n",
    "        all_labels.extend(labels.cpu().detach().numpy())\n",
    "        \n",
    "    val_accuracy = accuracy_score(all_preds, all_labels)\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "\n",
    "    return (val_accuracy, avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_dataloader, val_dataloader : DataLoader | None = None, epochs=5, learning_rate=1e-4):\n",
    "    results_dict = {\n",
    "        'time_per_batch': [],\n",
    "        'time_per_epoch': [],\n",
    "        'train_accuracy': [],\n",
    "        'train_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_loss': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'r2': [],\n",
    "    }\n",
    "    model = model.to(DEVICE)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    epoch_pbar = tqdm(range(epochs), desc=\"Training Epochs\")\n",
    "\n",
    "    model.train()\n",
    "    for epoch in epoch_pbar:\n",
    "        running_loss = 0.0\n",
    "        epoch_start = time()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for videos, labels in train_dataloader:\n",
    "            videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Store predictions and labels for metrics\n",
    "            all_preds.extend(outputs.argmax(dim=1).cpu().detach().numpy())\n",
    "            all_labels.extend(labels.cpu().detach().numpy())\n",
    "        \n",
    "        epoch_time = time() - epoch_start\n",
    "        results_dict['time_per_epoch'].append(epoch_time)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results_dict['train_accuracy'].append(accuracy_score(all_labels, all_preds))\n",
    "        results_dict['precision'].append(precision_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
    "        results_dict['recall'].append(recall_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
    "        results_dict['f1'].append(f1_score(all_labels, all_preds, average='weighted', zero_division=0))\n",
    "        results_dict['r2'].append(r2_score(all_labels, all_preds))\n",
    "        \n",
    "        avg_loss = running_loss / len(train_dataloader)\n",
    "        results_dict['train_loss'].append(avg_loss)\n",
    "        epoch_pbar.set_postfix(avg_loss=f\"{avg_loss:.4f}\")\n",
    "\n",
    "    val_acc, val_loss = infer_loop(model, val_dataloader, criterion)\n",
    "    results_dict['val_accuracy'].append(val_acc)\n",
    "    results_dict['val_loss'].append(val_loss)\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5073b2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 8, 256, 256]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "dataloader = get_dataloader(DATASET, subset_ratio=0.1, batch_size=16)\n",
    "x, y = next(iter(dataloader))\n",
    "print(x.shape, y.shape)\n",
    "del x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95223ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader =  get_dataloader(train_dataset, subset_ratio=None, batch_size=4)\n",
    "val_dataloader =  get_dataloader(val_dataset, subset_ratio=None, batch_size=4)\n",
    "test_dataloader =  get_dataloader(test_dataset, subset_ratio=None, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b38db4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   1%|          | 1/100 [00:52<1:26:46, 52.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 2.0465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   2%|▏         | 2/100 [01:40<1:21:47, 50.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 1.8105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   3%|▎         | 3/100 [02:30<1:20:21, 49.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 1.6554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   4%|▍         | 4/100 [03:15<1:17:04, 48.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Loss: 1.5096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   5%|▌         | 5/100 [03:58<1:13:01, 46.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 1.3938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   6%|▌         | 6/100 [04:41<1:10:39, 45.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 1.2755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   7%|▋         | 7/100 [05:26<1:10:01, 45.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Loss: 1.1657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   8%|▊         | 8/100 [06:12<1:09:27, 45.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Loss: 1.0716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   9%|▉         | 9/100 [06:55<1:07:38, 44.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Loss: 1.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  10%|█         | 10/100 [07:41<1:07:22, 44.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.9661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  11%|█         | 11/100 [08:20<1:04:08, 43.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Loss: 0.9019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  12%|█▏        | 12/100 [09:04<1:03:39, 43.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Loss: 0.8441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  13%|█▎        | 13/100 [09:54<1:05:56, 45.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100], Loss: 0.8164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  14%|█▍        | 14/100 [10:41<1:05:52, 45.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Loss: 0.7522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  15%|█▌        | 15/100 [11:21<1:02:35, 44.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Loss: 0.7469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  16%|█▌        | 16/100 [12:01<1:00:02, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Loss: 0.7062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  17%|█▋        | 17/100 [12:46<1:00:09, 43.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100], Loss: 0.6735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  18%|█▊        | 18/100 [13:35<1:01:43, 45.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100], Loss: 0.6469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  19%|█▉        | 19/100 [14:23<1:02:05, 45.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100], Loss: 0.6354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  20%|██        | 20/100 [15:03<58:45, 44.07s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Loss: 0.5593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  21%|██        | 21/100 [15:49<58:53, 44.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Loss: 0.5725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  22%|██▏       | 22/100 [16:34<58:07, 44.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100], Loss: 0.5674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  23%|██▎       | 23/100 [17:16<56:35, 44.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100], Loss: 0.5271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  24%|██▍       | 24/100 [18:03<56:46, 44.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100], Loss: 0.5189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  25%|██▌       | 25/100 [18:51<57:26, 45.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100], Loss: 0.4964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  26%|██▌       | 26/100 [19:38<56:45, 46.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100], Loss: 0.5085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  27%|██▋       | 27/100 [20:27<57:17, 47.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100], Loss: 0.4842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  28%|██▊       | 28/100 [21:12<55:49, 46.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100], Loss: 0.4478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  29%|██▉       | 29/100 [21:52<52:35, 44.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100], Loss: 0.4656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  30%|███       | 30/100 [22:31<49:56, 42.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100], Loss: 0.4339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  31%|███       | 31/100 [23:20<51:26, 44.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Loss: 0.4118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  32%|███▏      | 32/100 [24:06<51:00, 45.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100], Loss: 0.4193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  33%|███▎      | 33/100 [24:47<49:07, 44.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100], Loss: 0.4252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  34%|███▍      | 34/100 [25:30<47:49, 43.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Loss: 0.4070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  35%|███▌      | 35/100 [26:10<46:00, 42.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100], Loss: 0.3741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  36%|███▌      | 36/100 [26:53<45:34, 42.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100], Loss: 0.3951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  37%|███▋      | 37/100 [27:42<46:53, 44.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100], Loss: 0.3758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  38%|███▊      | 38/100 [28:27<46:04, 44.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100], Loss: 0.3449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  39%|███▉      | 39/100 [29:09<44:32, 43.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100], Loss: 0.3576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  40%|████      | 40/100 [29:52<43:45, 43.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100], Loss: 0.3088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  40%|████      | 40/100 [30:23<45:35, 45.60s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtraining_loop\u001b[39m\u001b[34m(model, train_dataloader, val_dataloader, epochs, learning_rate)\u001b[39m\n\u001b[32m     22\u001b[39m all_preds = []\n\u001b[32m     23\u001b[39m all_labels = []\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\ConvLSTMPyTorchLightning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\ConvLSTMPyTorchLightning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\ConvLSTMPyTorchLightning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mget_dataloader.<locals>.collate_fn\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m video, label, _ \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m      8\u001b[39m     video = video.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     video = \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvideo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mvideo\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m     videos.append(video)\n\u001b[32m     11\u001b[39m     labels.append(torch.tensor(label, dtype=torch.long))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\ConvLSTMPyTorchLightning\\.venv\\Lib\\site-packages\\pytorchvideo\\transforms\\transforms.py:30\u001b[39m, in \u001b[36mApplyTransformToKey.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]) -> Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     x[\u001b[38;5;28mself\u001b[39m._key] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\ConvLSTMPyTorchLightning\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\ConvLSTMPyTorchLightning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\ConvLSTMPyTorchLightning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\ConvLSTMPyTorchLightning\\.venv\\Lib\\site-packages\\pytorchvideo\\transforms\\transforms.py:74\u001b[39m, in \u001b[36mUniformTemporalSubsample.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m     70\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03m        x (torch.Tensor): video tensor with shape (C, T, H, W).\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpytorchvideo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform_temporal_subsample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_num_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_temporal_dim\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\ConvLSTMPyTorchLightning\\.venv\\Lib\\site-packages\\pytorchvideo\\transforms\\functional.py:41\u001b[39m, in \u001b[36muniform_temporal_subsample\u001b[39m\u001b[34m(x, num_samples, temporal_dim)\u001b[39m\n\u001b[32m     39\u001b[39m indices = torch.linspace(\u001b[32m0\u001b[39m, t - \u001b[32m1\u001b[39m, num_samples)\n\u001b[32m     40\u001b[39m indices = torch.clamp(indices, \u001b[32m0\u001b[39m, t - \u001b[32m1\u001b[39m).long()\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemporal_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = training_loop(MODEL, train_dataloader, val_dataloader, epochs=100, learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe37e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time_per_batch': [0.26043272018432617,\n",
       "  0.04736924171447754,\n",
       "  0.051451921463012695,\n",
       "  0.0431818962097168,\n",
       "  0.04414844512939453,\n",
       "  0.05073857307434082,\n",
       "  0.04169797897338867,\n",
       "  0.03714585304260254,\n",
       "  0.06663298606872559,\n",
       "  0.0694270133972168,\n",
       "  0.0866551399230957,\n",
       "  0.0837092399597168,\n",
       "  0.08386969566345215,\n",
       "  0.08619928359985352,\n",
       "  0.07990288734436035,\n",
       "  0.07819128036499023,\n",
       "  0.0991065502166748,\n",
       "  0.12497091293334961,\n",
       "  0.09679746627807617,\n",
       "  0.2366929054260254,\n",
       "  0.051824331283569336,\n",
       "  0.05388641357421875,\n",
       "  0.03834795951843262,\n",
       "  0.08256721496582031,\n",
       "  0.07532191276550293,\n",
       "  0.09441947937011719,\n",
       "  0.08146357536315918,\n",
       "  0.07831835746765137,\n",
       "  0.08380961418151855,\n",
       "  0.06888031959533691,\n",
       "  0.08284783363342285,\n",
       "  0.07780265808105469,\n",
       "  0.08165574073791504,\n",
       "  0.09861326217651367,\n",
       "  0.1150963306427002,\n",
       "  0.09218430519104004,\n",
       "  0.08516550064086914,\n",
       "  0.08716821670532227,\n",
       "  0.08283019065856934,\n",
       "  0.1100468635559082,\n",
       "  0.0975348949432373,\n",
       "  0.08039689064025879,\n",
       "  0.16640973091125488,\n",
       "  0.18262624740600586,\n",
       "  0.04330730438232422,\n",
       "  0.04842209815979004,\n",
       "  0.04947924613952637,\n",
       "  0.07048678398132324,\n",
       "  0.08787035942077637,\n",
       "  0.0824120044708252,\n",
       "  0.07271194458007812,\n",
       "  0.11232447624206543,\n",
       "  0.09030556678771973,\n",
       "  0.0910041332244873,\n",
       "  0.21477913856506348,\n",
       "  0.0318300724029541,\n",
       "  0.04415774345397949,\n",
       "  0.06953096389770508,\n",
       "  0.07536721229553223,\n",
       "  0.08398199081420898,\n",
       "  0.06954741477966309,\n",
       "  0.08271241188049316,\n",
       "  0.0903937816619873,\n",
       "  0.08214640617370605,\n",
       "  0.07098650932312012,\n",
       "  0.0824425220489502,\n",
       "  0.09008002281188965,\n",
       "  0.08405160903930664,\n",
       "  0.07313728332519531,\n",
       "  0.0932610034942627,\n",
       "  0.07769298553466797,\n",
       "  0.11199736595153809,\n",
       "  0.21616411209106445,\n",
       "  0.049721717834472656,\n",
       "  0.0476226806640625,\n",
       "  0.08109045028686523,\n",
       "  0.0764012336730957,\n",
       "  0.10254073143005371,\n",
       "  0.08787298202514648,\n",
       "  0.08613300323486328,\n",
       "  0.09113287925720215,\n",
       "  0.07823610305786133,\n",
       "  0.07808399200439453,\n",
       "  0.09990453720092773,\n",
       "  0.09226226806640625,\n",
       "  0.25723981857299805,\n",
       "  0.04772806167602539,\n",
       "  0.04450869560241699,\n",
       "  0.04524803161621094,\n",
       "  0.04516172409057617,\n",
       "  0.04342842102050781,\n",
       "  0.04499673843383789,\n",
       "  0.07568931579589844,\n",
       "  0.08433675765991211,\n",
       "  0.08308005332946777,\n",
       "  0.35269951820373535,\n",
       "  0.04379606246948242,\n",
       "  0.043411970138549805,\n",
       "  0.08555102348327637,\n",
       "  0.08087873458862305,\n",
       "  0.0931997299194336,\n",
       "  0.08649849891662598,\n",
       "  0.08657121658325195,\n",
       "  0.19847393035888672,\n",
       "  0.06441164016723633,\n",
       "  0.06186676025390625,\n",
       "  0.07463836669921875,\n",
       "  0.08718347549438477,\n",
       "  0.09045672416687012,\n",
       "  0.04454612731933594,\n",
       "  0.043421030044555664,\n",
       "  0.047446489334106445,\n",
       "  0.0474858283996582,\n",
       "  0.04224109649658203,\n",
       "  0.07107329368591309,\n",
       "  0.08473610877990723,\n",
       "  0.0882711410522461,\n",
       "  0.24205541610717773,\n",
       "  0.0440676212310791,\n",
       "  0.033625125885009766,\n",
       "  0.04820990562438965,\n",
       "  0.07537651062011719,\n",
       "  0.08550262451171875,\n",
       "  0.08713293075561523,\n",
       "  0.08144569396972656,\n",
       "  0.09121990203857422,\n",
       "  0.045506954193115234,\n",
       "  0.03391623497009277],\n",
       " 'time_per_epoch': [40.73285746574402, 46.82899618148804],\n",
       " 'train_accuracy': [0.7421875, 0.7578125],\n",
       " 'train_loss': [1.3638019962236285, 1.2699024295434356],\n",
       " 'val_accuracy': [0.71875],\n",
       " 'val_loss': [1.1200245767831802],\n",
       " 'precision': [0.7431808770621016, 0.7723319170042471],\n",
       " 'recall': [0.7421875, 0.7578125],\n",
       " 'f1': [0.7216402106371584, 0.7346057815019214],\n",
       " 'r2': [0.5465923931258452, 0.39644272885920007]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(results, orient=\"index\").T\n",
    "results_df.to_csv(\"3D_ResNet_Results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geture-research-ml (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
